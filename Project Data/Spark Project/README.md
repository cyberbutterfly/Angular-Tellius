Table of Contents
=================
 
   * [Change Logs](#change-logs)
   * [Prerequisite](#prerequisite)
   * [Building and Running Rest server](#building-and-running-rest-server)
   * [HDFS operations](#hdfs-operations)
   * [LOAD API](#load-api)
     * [1. Load CSV file.](#1-load-csv-file)
     * [2 . Load JSON file:](#2--load-json-file)
     * [3 . Load Parquet file:](#3--load-parquet-file)
     * [4 . Load XML file:](#4--load-xml-file)
     * [5 . Load from s3](#5--load-from-s3)
     * [6 . File upload:](#6--file-upload)
     * [7 . Load from mongodb](#7--load-from-mongodb)
     * [8 . Load from elasticsearch](#8--load-from-elasticsearch)
     * [9 . Load from URL :](#9--load-from-url-)
     * [10 . Load Telecom JSON file:](#10--load-telecom-json)
     * [11 . Load Telecom XML file:](#11--load-telecom-xml)
     * [12 . Load from Oracle](#12--load-from-oracle)
   * [VIEW API](#view-api)
     * [Schema](#-schema)
     * [data](#-data)
     * [metadata](#metadata)
     * [colStats](#colstats)
     * [uniquevaluecount](#uniquevaluecount)
     * [typestats](#typestats)
   * [SAVE API](#save-api)
   * [JOBS API](#jobs-api)
   * [TRANSFORM API](#transform-api)
   * [CONVERT API](#convert-api)
   * [Pipeline API](#pipeline-api)
   * [Schedule API](#schedule-api)
   * [Aggregate API](#aggregate-api)
   * [Indicator API](#indicator-api)
   * [Indicator AST Format](#indicator-ast-format)
   * [Signature API](#signature-api)
   * [Signature AST Format](#signature-ast-format)
   * [FUSION API](#fusion-api)
   * [List API](#list-api)
   * [Delete API](#delete-api)
   * [Config API](#config-api)
   * [KAFKA INSTALLATION](#kafka-installation)
   * [AZKABAN INSTALLATION](#azkaban-installation)
   * [STEPS TO ADD NEW FUNCTIONS TO OUR TOOL](#steps-to-add-new-functions-to-our-tool)

 

ChangeLog
============

* **Version 0.9.2**

    * Logistic regression support
    
    * Unseen label issue handled
    
    * Spark version upgraded to 1.6.1 
    
    * Added featurecolumns to evaluator options
    
    * Added featureimportance to evaluator options
    
    * ML List API provides more information about the model such as createdBy, algorithmtype, evaluationMetric
    
    * Signature and Indicator now have individual API to create, update, get, list, delete and apply signature/indicator.
    
    Breaking change:
    
    * Authentication :
        * Valid JWT token has to be passed for all the requests. It takes the same JWT token as that of Middleware
    
    * View API for typestats doesn't provide measures and dimension information. 

    * View API supports featurestats to provide measures and dimension information.

    * Function API has been removed and the signature, indicator and aggregate have their own API.

    * Transform API with transformationtype **addcolumn** doesn't support signature or indicator anymore
        Use Signature/Aggregate API to get the above functionality.
        
  


* **Version 0.9.1**

    * Load request change :
       * Location and external path parameters has been replaced by path in the options
    * Save request change :
       * Sync is added to save options : It takes a value true or false. If false, it returns an id using which the save job status can be tracked. If true, it runs the save job and returns the actual save response. By default its false.
    * File upload and Load from http doesn't take path as parameter.
       * Data will be saved with the value of name parameter to a default path specified by uploaded-pathprefix in application.conf
    * save path is removed and generated by dataset name itself
       * Data will be saved with the datasetname to a default path specified by save-pathprefix in application.conf.



Prerequisite
====================


* Java
* Mongongodb
* SBT
* mysql
* cassandra
* Spark


* Adding classpath to spark conf file

Open the file spark-env.sh inside spark installation folder (`/mnt/installation/spark-1.5.2-bin-hadoop2.6`). Add following line,

    export SPARK_CLASSPATH=<Path to application lib folder>   (current library path "/home/ubuntu/Sparkdev-01/target/pack/lib/*")
    
* Adding Oracle JDBC jars

Download the ojdbc14.jar from [Oracle site](http://www.oracle.com/technetwork/apps-tech/jdbc-10201-088211.html) and copy it into the application lib folder.

    (current library path "/home/ubuntu/Sparkdev-01/target/pack/lib/*")
    


* Setting up azkaban for scheduling : 

To install the azkaban refer [AZKABAN INSTALLATION](#azkaban-installation)

* In application.conf there is a configuration by name flowCreationDir, which will have the flow related files. Create a folder `lib` inside the path specified for that configuration

* Then go to the project directory and run 

        `sbt scheduler/clean scheduler/assembly`

* copy the scheduler-assembly-0.1.jar jar from scheduler/target/scala-2.10 inside the project to the lib folder you just created in above step


Building and Running Rest server
====================

 * Run the command `sbt clean pack`
 * Export applicationConf path using command `export applicationConf="<Path to application.conf file>"`
 * To run the rest server change directory(cd) to the spark folder.(cd `/mnt/installation/spark-1.5.2-bin-hadoop2.6`) 
   - If you want to run rest server locally,
     + run the command `bin/spark-submit --class com.tellius.rest.server.RestServer --master local <path to rest api jar>`
     + path of the rest api jar will be <path to our project>/target/pack/lib/rest-api_2.10-0.1.jar
   - If you want to run rest server on yarn
     +  run the command `bin/spark-submit --class com.tellius.rest.server.RestServer --master yarn-client <path to rest api jar>`
     + path of the rest api jar will be <path to our project>/target/pack/lib/rest-api_2.10-0.1.jar



Building and Running Rest server using script file
====================

 * Run the command `sbt clean pack`
 * Export applicationConf path using command `export applicationConf="<Path to application.conf file>"`
 * Export APPLICATION_HOME to the path to our project using command `export APPLICATION_HOME="<path to our project>"` 
     - If it is not set then the folder in which you run the script will be used as APPLICATION_HOME.
 * To run the rest server change directory(cd) to the $APPLICATION_HOME/target/pack/
   - If you want to run rest server locally,
     + run the command `bin/startServer.sh <Spark related configurations>`
     + If you want to provide separate file path for logfile you can give it in a option `--logfile <logfile path>`
   - If you want to run rest server on yarn
     +  run the command `bin/startServer.sh <Spark related configurations>`
     + If you want to provide separate file path for logfile you can give it in a option `--logfile <logfile path>`
     + If you want to run in local mode then you have to provide `--master local`, if you do not mention this option server starts in yarn-client mode.



HDFS operations
====================


* To use the cluster files should be in hdfs
* First cd to the folder where hadoop is installed.(cd `/mnt/installation/hadoop-2.6.0`)
* To create a directory in hdfs command is `bin/hdfs dfs -mkdir input`
* To list a files in hdfs command is `bin/hdfs dfs -ls input`
* To put a input file inside a hdfs folder `input` command is `bin/hdfs dfs -put <local file path> <hdfs-path>`
  - Example : `bin/hdfs dfs -put /home/ubuntu/input/test.json input`
* To remove a directory inside hdfs `bin/hdfs dfs -rmr <hdfs-path>`


Request Header
==============

All the API requires an **Authentication header** with a **valid JWT token** which is passed to the Middleware.

Using cUrl

```bash
    curl -v  -H "Content-Type: application/json" \
        -H "Authorization: Bearer eyJhbGciO...pUQXbeE1Q" \
        http://localhost:8080/
```

Using Postman

Header

 * Key : Authorization
 * Value : Bearer eyJhbGciO...pUQXbeE1Q 


Rest API provided as of now is,


LOAD API
====================


Load API loads structured data from different sources. File types supported by load API is,

* csv
* json
* parquet

Load rest api takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| sourcetype   |Type of the source. Legal values are csv, json, parquet,xml,mongodb,cassandra,elastic,twitter| Yes |
| options     | These are custom options to be passed for the data source. For example, {"header":"true"}     |  No |
| name | Human readable name for loaded datasource    |   No|
| payload | If data is getting uploaded, payload that has to be written into the file | No |
| kafkatopicname | kafka topic name for scheduled data | No |

**EXAMPLES FOR LOAD : **

1. Load CSV file.
-----------------------

* options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| delimiter    |  character from which columns are delimited | No | `,` |
| path | path of the csv file to be loaded | Yes | Nothing |
| header       |  when set to true the first line of files will be used to name columns and will not be included in data.| No | true |
| quote        |  but can be set to any character to represent a column. Delimiters inside quotes are ignored | No | `"` |
| dateFormat | string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat | No | None |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false | 


In the postman, give the input as specified below.

* Request URL -> api/load
* Click the Body tab
* Check the radio box raw.
* Change the content type to JSON(application/json)
* In the text box write the parameters which that api will take.

```
{
    "sourcetype":"csv",
    "options": {
      "header" : "true",
      "path":"spark_library/src/test/resources/testdata/airportdata/airports.csv",
      "copytosystem":"true"
    },
    "name":"something"
}
```

* click send button

Response will be a json which has an id, something like,

```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```

Save this id for future reference to that datasource.

2 . Load JSON file:
-----------------------

* Parameters:
    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| flatten | Boolean value to specify whether nested data has to be flattened. | No | true |
| path | path of the json file to be loaded | Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

Example:

```
{
    "sourcetype":"json",
    "name":"something",
    "options": {
      "flatten":"false",
      "path":"spark_library/src/test/resources/testdata/parquetData/test.json"
    }
}
```



3 . Load Parquet file:
--------
---------------
* Parameters:
    * options:
    
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| path | path of the parquet file to be loaded | Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

```
{
    "sourcetype":"parquet",
    "name":"something",
    "options":{"path":"spark_library/src/test/resources/testdata/parquetData/test.parquet"}
}
```

4 . Load XML file:
-----------------------

* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| flatten | Boolean value to specify whether nested data has to be flattened. | No | true |
| rowTag | Enclosing tag of each xml record | No | ROW |
| path | path of the xml file to be loaded | Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

Example:

```
{
    "sourcetype" :"xml",
    "name":"something",
    "options": {
      "rowTag" : "mdc",
      "flatten" : "false",
      "path":"/home/ganesh/Projects/Analytics/spark-tellius-project/spark_library/src/test/resources/testdata/xmldata/stats.xml"
    }
}
```

5 . Load from s3
-----------------------

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| path | path of the s3 file to be loaded | Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |
   
   To load from s3 same load api can be used but the path variable should be in the following format :
   
   `s3n://yourAccessKey:yourSecretKey@<yourBucket>/path/`
   
   Example:
   
   ```
   {
       "sourcetype":"csv",
       "options": {
         "header" : "true",
         "path":"s3n://yourAccessKey:yourSecretKey@mybucket/mypath/"
       },
       "name":"something"
   }
   ```


6 . File upload:
-----------------------


If you upload the file then content of the file you should **multipart/form-data** request to /load/file url.

Data will be saved with the datasetname to a default path specified by uploaded-pathprefix in application.conf

The request should contain parts
    * metadata - json data same as other requests containing file information. The following is an example for metadata
    
   ```{
       "sourcetype" :"json",
       "name":"sales",
       "options": {
            "flatten":"false"
       }
   }```
   
   * content - Content of the file as application/octet-stream

The below is the example using curl

      curl -X POST -H "Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW" -F 'metadata={"sourcetype" :"json","name":"something",        "options": {     "flatten":"false",     "header":"true"  } }' -F "content=@/home/madhu/Dev/tellius/Sparkdev-01/spark_library/src/test/resources/testdata/nestedJsonData/test.json" 'http://localhost:8080/api/load/file'
 
 
Please refer below picture for same in postman
 
![File upload in postman](images/fileupload.png "File upload in postman") 

Response will be a id which will refer to the data which is stored in the path.

```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```

7 . Load from mongodb
-----------------------


* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| host | Hostname of the mongo installation and its port (Ex : localhost:27017) | Yes |  | 
| database | name of the mongo database | Yes | | 
| collection | Name of the collection to be loaded | Yes | |
| credentials | Credentials for mongodb in the format user,database,password (Ex : telliususer,somedatabase,abc123) | Yes | |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

Example :

```
{
    "sourcetype" :"mongodb",
    "name":"something",
    "options":{ "host":"$mongoUrl",
                "database":"$database",
                "collection":"$collectionName",
                "credentials":"telliususer,somedatabase,abc123"}
}
```

----

**Note**

If the database name or table name provided is wrong then the error message will be,

database and table name combination specified may not present

----

8 . Load from cassandra
-----------------------

   * Request URL : api/load
   * Parameters:
   * options :
  
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| table |  Name of the table to be loaded from cassandra | Yes | Nothing |
| keyspace | Name of the key space in cassandra | Yes | Nothing |
| partitionKeyColumns |comma separated columns which is  responsible for data distribution accross your nodes. | No | None |
| clusteringKeyColumns |  comma separated columns which is responsible for data sorting within the partition. | No | None |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

```
{
    "sourcetype" :"cassandra",
    "name":"test",
    "options":{"table":"test", "keyspace":"staging"}
}
```


9 . Load from elasticsearch
-----------------------


Engine works with elasticsearch 2.1.1 version
To load from elasticsearch load api can be used :


* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| path | path of the content to be loaded in elastic search | Yes | Nothing |
| pushdown | Whether to translate (push-down) Spark SQL into Elasticsearch Query DSL | No | true | 
| strict | Whether to use exact (not analyzed) matching or not (analyzed) | No | false |
| double.filtering | Whether to tell Spark apply its own filtering on the filters pushed down (Works on spark-1.6 and higher)| No | false |
| es.nodes | List of Elasticsearch nodes to connect to | No | localhost |
| es.port | Default HTTP/REST port used for connecting to Elasticsearch - this setting is applied to the nodes in es.nodes that do not have any port specified | No | 9200 |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

There are more options from elastic search, for which please refer https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html

Example :

```
{
    "sourcetype" :"elastic",
    "name":"test",
    "options":{
      "path":"blog/user"
    }
}
```

10 . Load from URL :
-----------------------



This api used to load content from URL.

* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| httpurl | Url from which the contents has to be loaded | Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |

Data will be saved with the datasetname to a default path specified by uploaded-pathprefix in application.conf

Example : 

{
    "sourcetype" :"csv",
    "name":"something",
    "options": {
    "header":"true",
    "httpurl":"http://www.w3.org/2013/csvw/tests/test011/tree-ops.csv"
    }
}

11 . Load telecom JSON file
-----------------------

* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| path | path of the json file to be loaded | Yes | Nothing |
| flatten | Boolean value to specify whether nested data has to be flattened. | No | true |
| keycolumn           | Column name which contains counters based on which new columns has to be created. | Yes | Nothing |
| valuecolumn         | Values from this column will be the part of values for the newly created columns. |Yes | Nothing |
| groupingcolumn      | Column name which will be used for grouping and aggregating values.|Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

For loading json with specialoption, json data has to be structured similar to the structure shown below.

* keycolumn should be a string array and can be nested inside a struct and should not be nested inside any array
* valuecolumn and groupingcolumn should be nested inside array of objects ("mi.mv" in this case)

Sample schema

```
{"mi": 
  { "mts":"",
    "mt":[""],
    "mv":[ 
          { 
            "r":[""],
            "sf":true,
            "moid":"" 
          }
        ],
    "gp":900
  }
}
```

Example:

```
{
    "sourcetype" :"telecom_json",
    "name":"something",
    "options": {
      "flatten":"false",
       "keycolumn":"mi.mt",
       "valuecolumn":"mi.mv.r",
       "groupingcolumn":"mi.mv.moid",
       "path":"spark_library/src/test/resources/testdata/nestedJsonData"
    }
}
```


12 . Load telecom XML file
-----------------------

* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| path | path of the xml file to be loaded | Yes | Nothing |
| flatten | Boolean value to specify whether nested data has to be flattened. | No | true |
| rowTag | Enclosing tag of each xml record | No | ROW |
| keycolumn           | Column name which contains counters based on which new columns has to be created. | Yes | Nothing |
| valuecolumn         | Values from this column will be the part of values for the newly created columns. |Yes | Nothing |
| groupingcolumn      | Column name which will be used for grouping and aggregating values.|Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

Example:

```
{
    "sourcetype" :"telecom_xml",
    "name":"something",
    "options": {
      "rowTag" : "md",
      "flatten":"false",
      "keycolumn":"mi.mt",
      "valuecolumn":"mi.mv.r",
      "groupingcolumn":"mi.mv.moid",
      "path":"/home/ganesh/Projects/Analytics/spark-tellius-project/spark_library/src/test/resources/testdata/xmldata/stats.xml"
    }
}
```

13 . Load from Oracle
-----------------------


* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| url | Oracle database URL to connect to. | Yes | Nothing |
| dbtable | Oracle tablename that should be read. | Yes | Nothing |
| driver  | Oracle JDBC class name. | Yes | Nothing |
| user  | Username to Oracle database | Yes | Nothing |
| password  | Password to Oracle database | Yes | Nothing |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
| copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |

Example:

```
{
    "sourcetype" :"jdbc",
    "name":"something",
    "options": {
      "url" : "jdbc:oracle:thin:@hostname:1521:orcl",
      "dbtable":"tablename",
      "driver":"oracle.jdbc.driver.OracleDriver",
      "user":"username",
      "password":"password"
    }
}
```

13 . Load from Twitter
----------------------

   * Request URL : api/load
   * Parameters:


|  Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| sourcetype | example: "sourcetype": "twitter" | Yes |
| name | Human readable name for loaded datasource | Yes |


* Parameters:

    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| path | where the tweet data has to be stored | Yes | Nothing |
| filters | filter criteria for tweets. | No | Nil |
| numTweetsToCollect | Minimum number of tweets to collect. | No | 10 |
| intervalInSeconds | Write out a new set of tweets every interval (batch interval) | No | 10 |
| partitionsEachInterval  | The number of output files writen for each interval. | No | 1 |
| cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |


Example:

```
{
    "sourcetype" :"twitter",
    "name":"twitterfeed",
    "options": {
      "filters" : "data",
      "numTweetsToCollect":"100",
      "path":"/tmp/twitterdata/"
    }
}
```

**Note**
The consumer key, consumer secret key, the access token, and the access secret from the users twitter application
and the default values for numTweetsToCollect, intervalInSeconds and partitionsEachInterval are configured in application.conf.
example:

twitter-config {
    twitter.consumerKey="T19fOuPyHkJLEZDdaUgUZkJAt"
    twitter.consumerSecret="hNLBrxiTGc0yl5Fo6HtTmEwlSlG3igsYZX17bLK0wDTdCsrcNG"
    twitter.accessToken="2334392862-EbVOA4zupUYt2eOJgbwh3uCGEH3vchdDJm7DBWD"
    twitter.accessTokenSecret="ZKb3SlISuOnjog2oIe4EIxlzyxydiqQb76bITEtQkgkHu"
    twitter.numTweetsToCollect="10"
    twitter.intervalInSeconds="10"
    twitter.partitionsEachInterval="1"
  }


14 . Load from Memsql
-----------------------

Configuration for memsql server should be provided in **conf/application.conf**.
 
| Configuration Name        | Description          |
| ------------- |:-------------:|
| memsql.host | Memsql master hostname |
| memsql.port | Memsql master port |
| memsql.user | Memsql username |
| memsql.password | Memsql password |
| memsql.defaultDatabase | Default database name, will be used when dbname parameter is not provided in the request options |
  


* Parameters:

     * options:
    
    | Option Name        | Description          | Mandatory  | Default Value |
    | ------------- |:-------------:| -----:|---------:|
    | dbname | Memsql database name | No | Name specified at memsql.defaultDatabase in application.conf |
    | tablename | Memsql tablename that should be read. | Yes | Nothing |
    | cache | Specify whether the loading data should be cached or not. Takes the value "true" or "false" | No | value of cache-load in cache-config (application.conf) |
    | copytosystem | Whether data should be copied into the system and read from it for further operations. Data would not be read from the source after copying it to the system | No | false |


Example:

```
{
    "sourcetype" :"memsql",
    "name":"memsqlload",
    "options": {
          "dbname":"test",
          "tablename":"Persons"
        }
}
```



VIEW API
====================

API to return information about registered dataset

View Api takes following paremeters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id |The source that to be viewed | Yes |
| viewtype |Information to be viewed . Legal values schema, data,metadata, colstats | Yes |
| options | custom option to be passed based on viewtype. For colstats viewtype, options should contain colName key. | Yes |



Schema
-----------------------

 This will return the json with information about each field and its datatype.

 * Request URL : api/view
 * Parameters  :
```
 {
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"schema"
}
```
Here id field takes the value returned by load API

---
Example Response:
```
{
  "schema": {
    "fields": [
      {
        "name": "iata",
        "dataType": "StringType",
        "fields": []
      },
      {
        "name": "airport",
        "dataType": "StringType",
        "fields": []
      },
      {
        "name": "city",
        "dataType": "StringType",
        "fields": []
      },
      {
        "name": "state",
        "dataType": "StringType",
        "fields": []
      },
      {
        "name": "country",
        "dataType": "StringType",
        "fields": []
      },
      {
        "name": "lat",
        "dataType": "StringType",
        "fields": []
      },
      {
        "name": "long",
        "dataType": "StringType",
        "fields": []
      }
    ]
  }
}
```
---


data
-----------------------

 This will return the json with the sample data.

 * Request URL : api/view
 * Parameters  :
 * options :
 
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| nrows | How many rows of data needs to be fetched. | No | 200 |
| columnnames | name of the columns to which statistical data needs to be calculated separated by `","` or `all` | No | `all` |

```
 {
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"data",
    "options": {"nrows":"5","columnnames":"all"}
}
```

Here id field takes the value returned by load API

If the options is not provided then it'll take default number of rows.

---
Example Response:
```
{
  "data": [
    {
      "city": "Bay Springs",
      "state": "MS",
      "country": "USA",
      "long": "-89.23450472",
      "iata": "00M",
      "airport": "Thigpen ",
      "lat": "31.95376472"
    },
    {
      "city": "Livingston",
      "state": "TX",
      "country": "USA",
      "long": "-95.01792778",
      "iata": "00R",
      "airport": "Livingston Municipal",
      "lat": "30.68586111"
    },
    {
      "city": "Colorado Springs",
      "state": "CO",
      "country": "USA",
      "long": "-104.5698933",
      "iata": "00V",
      "airport": "Meadow Lake",
      "lat": "38.94574889"
    },
    {
      "city": "Perry",
      "state": "NY",
      "country": "USA",
      "long": "-78.05208056",
      "iata": "01G",
      "airport": "Perry-Warsaw",
      "lat": "42.74134667"
    },
    {
      "city": "Hilliard",
      "state": "FL",
      "country": "USA",
      "long": "-81.90594389",
      "iata": "01J",
      "airport": "Hilliard Airpark",
      "lat": "30.6880125"
    }
  ]
}
```
---

metadata
-----------------------


 This will return the json with the metadata about the loaded data.For example number of rows in data.

 * Request URL : api/view
 * Parameters  :

```
 {
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"metadata"
}
```

Here id field takes the value returned by load API

---
Example Response:
```
{
  "metaData": {
    "size": 3375,
    "noOfColumns": 7
  }
}
```
---


colStats
-----------------------



This will return the json with the following information about the column given in the option.

1. Sum : Sum of all the entries of a given colummn
2. Count : Number of rows in a given column
3. Average : Average(mean) of a given column
4. Max : Maximum value in a given column
5. Min : Minimum value in a given column
6. nonNullCount : count of rows in which given column value is not null
7. nonNullPercentage : Percentage of non null values

    * Request URL : api/view
    * Parameters  :
        * options :
        
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames | name of the columns to which statistical data needs to be calculated separated by `","` or `all` | Yes | Nothing |

Example1:

```
{
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"colstats",
    "options":{"columnnames":"lat"}
}
```

Here id field takes the value returned by load API

In the airport.csv there is one field calles 'lat' which is numbers, so colstats will return the stats on that field.


---
Example Response:

```
{
  "colStats": [
    {
      "count": 3375,
      "nonNullCount": 3375,
      "columnName": "lat",
      "avg": 40.03873757087695,
      "min": 13.48345,
      "max": 9.5167,
      "nonNullPercentage": 100,
      "sum": 135130.7393017097
    }
  ]
}
```
In the above response still there is lot of lat value and count but i've not put because of space constraints.

Example2:
```
{
         "id" :"2969b0fd-149d-4c54-80a4-ed9098a98f74",
         "viewtype":"colstats",
         "options":{"columnnames":"salary,bonus"}
}

```
Response:
```
{
  "colStats": [
    {
      "count": 5,
      "nonNullCount": 5,
      "columnName": "salary",
      "avg": 5400,
      "min": 4000,
      "max": 6000,
      "sum": 185
    }
  ]
}

```

Example3:

  When you give the option `all` for columnnames, all columns should be numbers.

```
{
         "id" :"73a4d0d5-f299-45b5-9ac9-e2cf5c03fae8",
         "viewtype":"colstats",
         "options":{"columnnames":"all"}
}

```


Example4:

  
```
{
         "id" :"73a4d0d5-f299-45b5-9ac9-e2cf5c03fae8",
         "viewtype":"colstats",
         "options":{"columnnames":"name"}
}

```


In the above example name is a string column. So an error message will be displayed saying,

`colstats cannot be applied on column 'name', because its not a numeric type`


---

uniquevaluecount
-----------------------

This will return the array with the distinct value of a given column and its count.

    * Request URL : api/view
    * Parameters  :
    * options :
        
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames | name of the columns of which unique value count is needed, separated by `","` or `all` | No | "all" |
| numofranges | number of ranges unique values has to be counted on | No | 200 |


Example 1: Number of distinct values is lesser than 200

```
{
"id" :"$id",
"viewtype":"uniquevaluecount",
"options": {
    "columnnames":"age"
    }
}
```

Response:

```
{
    "uniquevaluecount": [{
      "columnName": "age",
      "stats": [{
        "start": "25",
        "end": "",
        "count": 21
      }, {
        "start": "35",
        "end": "",
        "count": 3
      }, {
        "start": "55",
        "end": "",
        "count": 4
      }, {
        "start": "81",
        "end": "",
        "count": 1
      }, {
        "start": "48",
        "end": "",
        "count": 1
      }]
    }]
  }
```

* start - signifies the value of the column
* end - will be empty
* count - will be number of rows having the value equals value specified in start parameter


Example 2: Number of distinct values is more than 200

```
{
"id" :"$id",
"viewtype":"uniquevaluecount",
"options": {
    "columnnames":"long",
    "numofranges":"10"
    }
}
```

Response:

```
{
    "uniquevaluecount": [{
      "columnName": "long",
      "stats": [{
        "start": "",
        "end": "-144.42",
        "count": 231
      }, {
        "start": "-144.42",
        "end": "-112.19",
        "count": 498
      }, {
        "start": "-112.19",
        "end": "-79.96",
        "count": 2195
      }, {
        "start": "-79.96",
        "end": "-47.73",
        "count": 448
      }, {
        "start": "-47.73",
        "end": "-15.5",
        "count": 0
      }, {
        "start": "-15.5",
        "end": "16.73",
        "count": 0
      }, {
        "start": "16.73",
        "end": "48.96",
        "count": 0
      }, {
        "start": "48.96",
        "end": "81.19",
        "count": 0
      }, {
        "start": "81.19",
        "end": "113.42",
        "count": 1
      }, {
        "start": "113.42",
        "end": "",
        "count": 3
      }]
    }]
  }
```

* start - signifies the minimum value of the range, will be empty for the first range
* end - signifies the maximum value of the range, will be empty for the last range
* count - number of values in the range
 
Note:

* unique value count returns empty stats when number of unique values for a Non-numeric column is more than 200.

Response:

```
{
    "uniquevaluecount": [{
      "columnName": "country",
      "stats": []
  }
```
---

typestats
-----------------------

 This will return the json with statistics about the type of the column. 

 * Request URL : api/view
 * Parameters  :
 * options :
 
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames | name of the columns to which statistical data needs to be calculated separated by `","` or `all` | No | `all` |

Example:

```
 {
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"typestats",
    "options": {"columnnames":"bonus,name"}
}
```


Response:

```
typeStats": [{
    "columnName": "bonus"
    "mainType": "Number",
    "otherTypePercentage": 40.0,
    "otherType": "Other",
    "mainTypePercentage": 40.0,
    "nullPercentage": 0.0
  }, {
    "columnName": "name"
    "mainType": "String",
    "otherTypePercentage": 20.0,
    "otherType": "Other",
    "mainTypePercentage": 80.0,
    "nullPercentage": 0.0
  }]
}
```

* **mainType** represents the type which most of the data is of
* **mainTypePercentage** represents the ratio of values of mainType
* **nullPercentage** represents the ratio of values which are null for that column
* **otherTypePercentage** represents the ratio of values which are not of mainType and null


featurestats
------------

This will return the json with information like featurecategory(text, categorical or continuos) and featuretype(measure or dimension) about the columns.  

 * Request URL : api/view
 * Parameters  :
 * options :
 
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames | name of the columns to which featurestats needs to be calculated separated by `","` or `all` | No | `all` |
| maxcategories | distinct value count of a column, below which it will be considered as categorical | No | 100 |


Example:

```
 {
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"featurestats",
    "options": {"columnnames":"bonus,name",
                 "maxcategories":"50"}
}
```

Response:


```
featurestats": [{
    "columnName": "bonus",
    "featureType": "dimension",
    "featureCategory":"categorical"
  }, {
    "columnName": "name",
    "featureType": "dimension",
    "featureCategory":"string"
  }]
}
```


Example:

```
 {
    "id":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "viewtype":"featurestats",
    "options": {"columnnames":"bonus,name",
                 "maxcategories":"50"}
}
```

Lets assume in this example unique value of bonus is greater than 75. So columnType will be "measure"

Response:


```
"featurestats": [{
    "columnName": "bonus",
    "featureType": "measure",
    "featureCategory": "continuous"
  }, {
    "columnName": "name",
    "featureType": "dimension",
    "featureCategory": "string"
  }]
}
```

* **featureType : dimension** (If mainType satisfies one of the below options)
    - It contains string less than 100 characters(max length) 
    - it contains numbers and Number of unique values are less than 75 
    - Date
    - Binary 
* **featureType : measure** (If mainType satisfies one of the below options)
    - It contains string greater than or equal to 100 characters(max length)
    - it contains numbers and Number of unique values greater than or equal to 75

**Note**
 100 and 75 are default values which can be configured in the application.conf Configuration file. 
 fieldstats-config {
    columnType.measure.String.minChars="100"
    columnType.measure.Number.minUniqueValues="75"
  }

* **featureCategory**
    - text - column of string whose maximum length is more than 100 characters or if the datatype is like blob etc
    - categorical - If the unique value count of mainType of a column is less than provided maxcategories then it is considered as categorical.
    - continuous - if the column is a number and the unique value count of mainType of a column is more than procided maxcategories then it is considered as continuous.


SAVE API
====================

API to save transformed data to external systems

Save API takes following parameters,

| Name | Description | Mandatory |
| ------------- |:-------------:| -----:|
| sourceid | The source that to be saved | Yes |
| outputtype | Type of the file . Legal values json, parquet, txt , mongodb, cassandra,elastic | Yes |
| options | Based on the outputtype option varies. For example in csv it can be header is available or not. For mongodb options are host,collection,database| No |

Common options for all outputtype

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| delay | true or false. If true save will happen during scheduled period. It will not happen as soon as call goes| No | false |
| datasetname | Human readable name for dataset.| Yes  | Nothing |
| sync | true or false. If false, it returns an id using which the save job status can be tracked. If true, it runs the save job and returns the actual save response | No | false |


Save API(sync = true) returns the below structure if it was able to save the data successfully
 
```
{
  "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
  "status":"SUCCESS",
  "starttime":"Fri Apr 22 06:00:00 IST 2016",
  "timetaken":"10s",
  "result":{
    "sourceid": "e0e9e53f4d574a16a9a0656eb2473e9d"
  }
}
```
Save API(sync = true) returns the below structure if it failed to save the data successfully

```
{
  "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
  "status":"FAILURE",
  "starttime":"Fri Apr 22 06:00:00 IST 2016",
  "reason":"requirement failed: Output path already exists",
}
```


Save API(sync = false) returns the job id, which can be used to track the status of the save job

```
{
  "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d"
}
```

Save API result would contain a field **sourceId**. This sourceid represents the datasetId to load the saved data. 

**sourceId** returned from the Save API is an alphanumeric string (doesn't contain any special characters like -) unlike other responses which would be an UUID.


* Save to txt file :

    * Request URL : api/save
    * Parameters:
    * options:
       
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| delimiter    |  character from which columns are delimited | No | `,` |
| header       |  when set to true the first line of files will be used to name columns and will not be included in data.| No | true |
| quote        |  but can be set to any character to represent a column. Delimiters inside quotes are ignored | No | `"` |
| savemode | mode in which the file has to be saved | No | ErrorIfExists |

* savemode : valid values are
  - ErrorIfExists : It will give error if the path already exists
  - Overwrite : It will overwrite the old file if exists
  - Append : It will append to the old file if exists
  
Data will be saved with the datasetname to a default path specified by save-pathprefix in application.conf.


```
    {
          "sourceid":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
          "outputtype" :"txt",
          "options":{"datasetname":"saved_csv"}
    }
```

* Save to json file
    * Request URL : api/save
    * Parameters:

Data will be saved with the datasetname to a default path specified by save-pathprefix in application.conf

```
    {
              "sourceid":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
              "outputtype" :"json",
              "options":{"datasetname":"saved_json"}
    }
```

* Save to parquet file
    * Request URL : api/save
    * Parameters:
    * options:
               
    | Option Name        | Description          | Mandatory  | Default Value |
    | ------------- |:-------------:| -----:|---------:|
    | partitionby | Columnnames to partition parquet files on. Comma separated values to partition on multiple columns| No  | Empty |

Data will be saved with the datasetname to a default path specified by save-pathprefix in application.conf    

```
    {
              "sourceid":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
              "outputtype" :"parquet",
              "options":{"partitionby":"state,country", datasetname":"saved_parquet"}
    }
```

* Save to mongodb

    * Request URL : api/save
    * Parameters:
    * options :
    
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| host    |  this parameter takes host:port of the mongodb.| Yes | Nothing|
| database       |   Name of the database|  Yes | Nothing|
| collection        |   Name of the collection in mongodb|  Yes | Nothing|

```
    {
              "sourceid":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
              "outputtype" :"mongodb",
              "options":{"host":"localhost:27017", "database":"staging", "collection":"airportData_", datasetname":"saved_mongodb"}
    }
```


* Save as xml file
    * Request URL : api/save
    * Parameters:
    * options :
        
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| rowTag | The row tag of your xml files to treat as a row | No | ROW |
| rootTag | The root tag of your xml files to treat as the root. | No | ROWS |

Data will be saved with the datasetname to a default path specified by save-pathprefix in application.conf

Example:

```
{
    "sourceid":"$id",
    "outputtype" :"xml",
    "options": {"rowTag" : "mdc", "rootTag":"mi", datasetname":"saved_mongodb"}
}
```


* Save to cassandra


   * Request URL : api/save
   * Parameters:
   * options :
  
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| table |  Name of the table to be created in cassandra | Yes | Nothing |
| keyspace | Name of the key space to be created in cassandra | Yes | Nothing |
| partitionKeyColumns |comma separated columns which is  responsible for data distribution accross your nodes. | No | None |
| clusteringKeyColumns |  comma separated columns which is responsible for data sorting within the partition. | No | None |
| createtable | If true, creates cassandra table. If false, skips creation | No | true |


```
{
    "sourceid":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
    "outputtype" :"cassandra",
    "options":{"table":"test", "keyspace":"staging", "datasetname":"saved_cassandra", "createtable":"false"}
}
```

* Save to elasticsearch 
    * Request URL : api/save
    * Parameters: 
    * options :
            
    | Option Name        | Description          | Mandatory  | Default Value |
    | ------------- |:-------------:| -----:|---------:|
    | path | Index to store in elasticsearch | Yes | Nothing |
        
```
    {
              "sourceid":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
              "outputtype" :"elastic",
              "options":{"path":"user/ganesh", datasetname":"saved_elasticsearch"}
    }
```


* Save to memsql 
    * Request URL : api/save
    
    Configuration for memsql server should be provided in **conf/application.conf**.
     
    | Configuration Name        | Description          |
    | ------------- |:-------------:|
    | memsql.host | Memsql master hostname |
    | memsql.port | Memsql master port |
    | memsql.user | Memsql username |
    | memsql.password | Memsql password |
    | memsql.defaultDatabase | Default database name, will be used when dbname parameter is not provided in the request options |
    
    * Parameters:
    
         * options:
        
    | Option Name        | Description          | Mandatory  | Default Value |
    | ------------- |:-------------:| -----:|---------:|
    | dbname | Memsql database name | No | Name specified at memsql.defaultDatabase in application.conf |
    
```
    {
              "sourceid":"bcf6b469-92f2-4473-baf8-7cdba8469ecf",
              "outputtype" :"memsql",
              "options":{
                "dbname":"test",
                "datasetname":"saved_memsql"
              }
    }
```



JOBS API
====================

API to track the status of the asynchronous jobs
Jobs API takes jobid returned when job was submitted (through api/save). 

* Job status
    * Request method : GET
    * Request URL : api/jobs/$jobId
    
 Response would contain following parameters.
 
| Name        | Description          | Mandatory?  |
| ------------- |:-------------:| -----:|
| jobId | Job id passed to the API  | Yes |
| status | Current status of the job. Valid values are SUCCESS, FAILURE, RUNNING | Yes |
| starttime | Time at which job started running | Yes |
| timetaken | Time taken by the job to finish. In seconds. | Yes, when job has SUCCESSFULLY completed |
| reason | Reason for Job failure | Yes, when job has NOT SUCCESSFULLY completed |
| result | Actual result of the job | Yes, when job has SUCCESSFULLY completed |

**Scenarios**:

If the job is still running

```
 {
   "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
   "status":"RUNNING",
   "starttime":"Fri Apr 22 06:00:00 IST 2016"
 }
```

If the job has successfully completed

```
 {
   "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
   "status":"SUCCESS",
   "starttime":"Fri Apr 22 06:00:00 IST 2016",
   "timetaken":"10s",
   "result":{
     "sourceid": "e0e9e53f-4d57-4a16-a9a0-656eb2473e9d"
   }
 }
```

If the job has failed

```
 {
   "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
   "status":"FAILURE",
   "starttime":"Fri Apr 22 06:00:00 IST 2016",
   "reason":"requirement failed: Output path already exists",        
 }
```
 
 * Jobs list
    * Request method : GET
    * Request URL : api/jobs/list
   
Returns the list of runnning jobs

Response will be 

```
"results" : [{
            "jobId":"e0e9e53f-4d57-4452-a9a0-656eb2473e9d",
            "status":"RUNNING",
            "starttime":"Fri Apr 21 06:00:00 IST 2016"
        },
        {
            "jobId":"e0e9e53f-4d57-4a16-a9a0-656eb2473e9d",
            "status":"RUNNING",
            "starttime":"Fri Apr 22 06:00:00 IST 2016"
        }]
```
   
   
   


TRANSFORM API
====================

API to transform loaded data to as per requirements

Transform API takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| sourceid | Id of the source that has to be transformed  | Yes |
| transformationtype | Type of the transformation. Legal values are columnselect, cast, sort, filter, deletecolumn, handlenull, sample, handlecolumns | Yes |
| options | Based on the transformationtype option varies | No |

Transform API returns an id which will be similar to load API, later that id can be used to refer the transformed data.

```
{
  "sourceid": "e0e9e53f-4d57-4a16-a9a0-656eb2473e9d"
}
```

**Transformation Type**

* columnselect
    * Request URL : api/transform
    * Parameters :
    * options:
     
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames | It takes column names of dataframe separated by , | Yes | Nothing |
| select | true or false, if true column names specified in the option columnnames will be selected, if false column names which are not present in the option columnnames will be selected in resulting dataframe. | Yes | Nothing |

```
{
         "sourceid" :"6e80bbde-5ff8-4536-a6de-c611de69c512",
         "transformationtype":"columnselect",
         "options":{"select":"true","columnnames":"lat"}
}
```

* cast:
    * Request URL: api/transform
    * Parameters:
    * options:
   
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames | Comma separated column names of a dataframe whose type has to be casted. | Yes | Nothing |
| dataType | datatype to which it has to be casted. Legal values are `string`, `boolean`, `decimal`, `date`.| Yes | Nothing |
| format | In which format the date is present in the given column | No |  `yyyy-MM-dd HH:mm:ss` |

Example1:

```
{
         "sourceid" :"$id",
         "transformationtype":"cast",
         "options":{"dataType":"decimal","columnname":"lat,long"}
}
```

Example2:

```
{
          "sourceid" :"$id",
          "transformationtype":"cast",
          "options":{"columnname":"datefield", "datatype":"date", "format":"yyyyMMdd"}
}
```

* sort:
    * Request URL : api/transform
    * Parameters :
    * options:
        
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnname |For which column sorting should be applied.| Yes | Nothing |
| sortOrder | In what order dataframe has to be sorted. Legal values are, `asc`, `desc`| Yes | Nothing |


```
{
         "sourceid" :"$id",
         "transformationtype":"sort",
         "options":{"sortOrder":"asc","columnname":"salary"}
}
```

* filter:
    * Request URL : api/transform
    * Parameters :
    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| condition |condition based on which dataframe needs to filterd.| Yes | Nothing |

```
{
         "sourceid" :"$id",
         "transformationtype":"filter",
         "options":{"condition":"salary>4000"}
}
```

* Conditions for different cases:
    * For text
        * Equals ->  "city = 'Bay Springs'" (city equals to Bay Springs )
        * Does not equals -> "city != 'Bay Sprngs'" (city Does not equals to Bay Sprngs )
        * contains -> "city like '%Bay Sprin%'" (city contains Sprin )
        * does not contain -> "city not like '%Bay Sprin%'" (city does not contains Sprin )
        * contains but does not contain -> "city  like '%Bay%' and city not like '%Livingston%'"
        * ends with -> "city  like '%Springs'" (city ends with Springs)
        * not ends with -> "city  not like '%Springs'" (city not ends with Springs)"
        * starts with -> "city  like 'Bay%'" (city starts with Bay)"
        * not starts with -> "city not like 'Bay%'" (city not starts with Bay)"
        * does not being with ->
        * blank -> "city = ''" (city is blank)
        * not blank -> "city != ''" (city is not blank)
        * matches regex -> "city  regexp '^[A-Za-z]+$'" (city which have only alphabets, no special char, no space, no numbers)
    * For numbers
        * Greater than -> "salary > 5000" (salary greater than 5000)
        * Greater than or equal to -> "salary >= 50000" (salary greater than or equal to 5000)
        * less than "salary < 50000" (salary less than 5000)
        * Less than or equal to "salary <= 50000" (salary less than or equal to 5000)
        * inbetween -> "salary>=3000 and salary<=5000" (salary inbetween 3000 and 5000)
        * Filter across multiple columns
        * and -> "salary >= 3000 and bonus = 30"
        * or -> "salary>=3000 and bonus = 30"
    * Math functions
        * Add -> "salary+1000>=3000"  (Add 1000 to salary before comparing)
        * Multiple -> "salary*1000>=3000" (Multiply 1000 before comparing)
        * divide by number -> "salary/10<=3000" (devide 10 before comparing)
        * RoundUp -> ceil(bonus) = 46
        * Round Down -> floor(bonus) = 45



*  deletecolumn:
    * Request URL : api/transform
    * Parameters :
    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames |comma separated column names which has to be deleted.| Yes | Nothing |


```
{
         "sourceid" :"$id",
         "transformationtype":"deletecolumn",
         "options":{"columnnames":"lat"}
}
```

* handlenull
    * Request URL : api/transform
    * Parameters :
    * options:
    
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames |Name of the columns whose null values to be handled or `all` if you want to handled for all the columns.| Yes | Nothing |
| handlewith | How the null values to be handled. Legal values are, `mean`,`zero`,`ignore` and `"some_value"` | Yes | Nothing |

         * handlewith: 
            * mean: replaces null value of the field with its mean.
            * zero: replaces null value of the field with 0.
            * ignore: remove the row containing null value for that row.
            * "some_value":replaces the null value with the "some_value"

Example1:
```
{
         "sourceid" :"$id",
         "transformationtype":"handleNull",
         "options":{"columnnames":"salary,bonus","handlewith":"mean"}
}
```

Example2:
```
{
         "sourceid" :"$id",
         "transformationtype":"handlenull",
         "options":{"columnnames":"all","handlewith":"5850"}
}
```

Example2:
```
{
         "sourceid" :"$id",
         "transformationtype":"handlenull",
         "options":{"columnnames":"name","handlewith":"mean"}
}
```
In the above example name is the string column. So mean cannot be calculated on string column. So an error message will 
be given saying handleNull with mean cannot be applied on column '$columnName', because its not a numeric type


* handlecolumns
    * Request URL : api/transform
    * Parameters :
    * options:
    
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames |Name of the columns whose values to be handled or `all` if you want to handled for all the columns.| Yes | Nothing |
| handlewith | How the values to be handled. Legal values are, `mean`,`zero`,`ignore` and `"some_value"` | Yes | Nothing |
| columntype | handle type of the values in a given column. If in the column only few values are of type string and rest are all double, you may want to handle those string values. At that time value will be 'string'| No | string |

         * handlewith: 
            * mean: replaces value of the field with its mean.
            * zero: replaces value of the field with 0.
            * ignore: remove the row containing columntype value for that row.
            * "some_value":replaces the value with the "some_value"

Example1:
```
{
"sourceid" :"$id",
"transformationtype":"handlecolumns",
"options":{"columnnames":"salary,bonus","handlewith":"zero","columntype":"string"}
}
```


In above example salary should be of a double type. But because it has few string anamolies, spark finds it datatype is a string. So we are replacing the string values in the column with zero.


* splitcolumn
    * Request URL : api/transform
    * Parameters :
    * options :
    
 | Option Name        | Description          | Mandatory  | Default Value |
 | ------------- |:-------------:| -----:|---------:|
 | columnname |Name of the column which should be split into multiple column.| Yes | Nothing |
 | delimiter | delimiter based on which split will be done. | Yes | Nothing |
 
 splitcolumn API is supported on columns of type String only.
 
        Supported delimiters are

        * Normal delimiters: 
        ```
        "," ";" "-" "="
        ```
        * Special delimiters: Special delimiters should preceded by a single or double escape character \
        ``` 
        "\\" "\/" "\"" "\'" "\n" "\t" "\\|"
        ```

          Destination column names are named using convention sourceColumnName_c1, sourceColumnName_c2 ...

Example:

```
{
         "sourceid" :"8fd83874-379a-4e55-950f-b1aaa43c3d24",
         "transformationtype":"splitcolumn",
         "options":{"columnname":"name","delimiter":","}
}
```

* splitrows
    * Request URL : api/transform
    * Parameters :
    * options :
        
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnname |Name of the column which should be split into multiple rows.| Yes | Nothing |
| delimiter | delimiter based on which split will be done. | Yes | Nothing |
 
        Supported delimiters are

        * Normal delimiters: 
        ```
        "," ";" "-" "="
        ```
        * Special delimiters: Special delimiters should preceded by a single or double escape character \
        ``` 
        "\\" "\/" "\"" "\'" "\n" "\t" "\\|"
        ```


Example:

```
{
         "sourceid" :"8fd83874-379a-4e55-950f-b1aaa43c3d24",
         "transformationtype":"splitrows",
         "options":{"columnname":"name","delimiter":","}
}
```

Response will be a json which has an id, something like,
```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```



* mergecolumns
    * Request URL : api/transform
    * Parameters :
    * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames |Name of the columns which should be merged into single column.| Yes | Nothing |
| mergedcolumnname |Name which should be given to merged column. | No | column names to be merged with _ as delimiter. |

Example:

```
{
         "sourceid" :"8fd83874-379a-4e55-950f-b1aaa43c3d24",
         "transformationtype":"mergecolumns",
         "options":{"columnnames":"name,salary",    "mergedcolumnname":"namewithsalary"}
}
```
Response will be a json which has an id, something like,
```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```
This is the id of the datasource which is returned after transfomation, which can be used to do some more operation on that.

* renamecolumn:
     * Request URL : api/transform
     * Parameters :
     * options:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnname |column name to be renamed| Yes | Nothing |
| renameto |new name to be given for that column | Yes | Nothing |

Example:

```
{
         "sourceid" :"$id",
         "transformationtype":"renamecolumn",
         "options":{"columnname":"lat","renameto":"latitude"}
}
```

* addcolumn
    * Request URL : api/transform
    * Parameters :
    * options:
    
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columntype |Type of the column to be added. Accepted values are fixedstring, subset| Yes | Nothing |
| fixedstring |Fixed string which has to be the value of the new column| Yes (if columnType is fixedstring) | Nothing |
| subset |Subset function | Yes (if columnType is subset) | Nothing |
| columnname |Column name to be given for the newly added column| Yes | Nothing |

Subset functions available are:
**String functions**

  * left(columnname, 2) : Takes the part of string till 2nd position
  * right(columnname, 5) : Takes the part of string from 5th position till the end of string
  * mid(columnname, 3,5) : Takes the part of string from 5th position till the end of string

**Date functions:** 

Date functions can be used only if the columnname being passed has the date string formatted in 'yyyy-MM-dd HH:mm:ss' Example :2015-06-23 22:30:00

  * minute(columnname) : Extract minute of the timestamp.
  * hour(columnname) : Extract hour of the timestamp.
  * day(columnname) : Extract day of month of the timestamp.
  * date(columnname) : Extract date part from the timestamp.
  * month(columnname) : Extract month of the timestamp.
  * dayofweek(columnname) : Extract the day of the week from timestamp. Ex : 1 for Sunday,2 for Monday etc.

Example 1: Fixed string
```
{
         "sourceid" :"8fd83874-379a-4e55-950f-b1aaa43c3d24",
         "transformationtype":"addcolumn",
         "options":{"columntype":"fixedstring", "fixedstring":"USA", "columnname":"country"}
}
```

Example 2: Subset

For this function column on which you call the subset function should be of format `yyyy-MM-dd` or 'yyyy-MM-dd HH:mm:ss'.
If not first use the cast transform api to cast the field to date then use that column in these subset function.

```
 {
         "sourceid" :"8fd83874-379a-4e55-950f-b1aaa43c3d24",
         "transformationtype":"addcolumn",
         "options":{"columntype":"subset", "subset":"month(datefield)", "columnname":"month"}
 }
```

Response will be a json which has an id, something like,
```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```
This is the id of the datasource which is returned after transfomation, which can be used to do some more operation on that.

----


* findandreplace:

    * Request URL : api/transform
    * Parameters :
    * options:
    
| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
| columnnames  | Name of the columns which has to be selected in the joined table or `all`  | Yes |
| find | Find the content in the column which has to be replaced | Yes | Nothing|
| replace| Replace the found value with the value of this parameter | Yes| Nothing |

Example:

```
{
"sourceid" :"$id",
"transformationtype":"findandreplace",
"options":{"columnnames":"country","find":"USA","replace":"INDIA"}
}
```

CONVERT API
====================

API to convert the data from one format to other format. To achieve this, you need to call load and save API's in combination.

* Convert data from csv format to json format
    * Step 1 : Load CSV data using load Api
        * Request URL : api/load
        * Parameters:

```
{
    "sourcetype":"csv",
    "options": {
      "header" : "true",
      "path":"spark_library/src/test/resources/testdata/airportdata/airports.csv"
    },
    "name":"something"
}
```

Response will be a json which has an id, something like,

```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```

  * Step 2 : Save Json data of the earlier loaded using saveAPI
    * Request URL : api/save
    * Parameters:

```
    {
          "sourceid":"087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b",
          "outputtype" :"json",
          "options": {
                "header" : "true",
                "datasetname" : "saved_csv",
                "path":"spark_library/src/test/resources/testdata/output"
          }
    }
```


Pipeline API
====================

This API is used to pipe different requests.

* Request URL : pipeline

There are API's to create, update, view or delete the pipeline request.

**Pipeline create**

This api is used to create pipeline from set of requests. 

* Request URL : pipeline/create
* Parameters :

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| sourceid | Parameter to specify a source to the pipeline. Can be overridden in Pipeline execute API | Yes |
| name | Name of the pipeline request | Yes |
| stages | Array of configuration for stages | Yes |

**Pipeline stage**

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| stagetype | Type of the operation being performed in stage. Accepted values are transform, fusion, split, save | Yes |
| parentIds | Array of sourceids for the stage. This can be used to override the sourceid provided in the first task request | Yes |
| taskIds | Array of taskId i.e request ids in this pipeline stage | Yes |

Each stagetype should have only its associated tasks.

**Transform stage**

Transform stage can contain multiple Transform. In taskids configured, output of one request will be piped to next request.

```
{
  "type":"transform",
  "parentIds":["457a2997-65f3-4cf0-8999-0de2270f4aca"],
  "taskIds":["c6f60b41-2ccb-426a-8487-01e405f4fd87","5c7cdf41-91e9-4f07-9075-b26ae0fd51d4"]
}
```

**Fusion stage**

Fusion stage can contain single Fusion request.

```
{
  "type":"fusion",
  "parentIds":["a9b8490b-8da1-489b-b15d-7ff0a9fb0237","5fee3000-fdd1-4ce4-b979-982a6c99565a"],
  "taskIds":["03ba0378-b8f0-4045-b0fa-3f3fcf163de0"]
}
```

**Save stage**

Save stage can contain single Save request.

```
{
  "type":"save",
  "parentIds":[],
  "taskIds":["61d69f9e-9284-4ef1-bbb0-7fc26e5e24cc"]
}
```

**Split stage**
Split stage can contain the ids of ML feature transform request(transformationtype = sampleratio). Sample ratio API is used to sample the dataset into 2. The ids returned from the Sample ratio API must be specified in taskids. 

```
{
  "type":"save",
  "parentIds":[],
  "taskIds":["61d69f9e-9284-4ef1-bbb0-7fc26e5e24cc","62d69f9e-9284-4ef1-bbb0-7fc26e5e24cc"]
}
```

**MLTrain stage**
MLTrain stage can contain a id of ML TrainPipeline request. Pass the delay:true configuration while creating MLTrainPipeline.

```
{
  "type":"mltrain",
  "parentIds":[],
  "taskIds":["61d69f9e-9284-4ef1-bbb0-7fc26e5e24cc"]
}
```

**MLPredict stage**
MLPredict stage can contain a id of Predict request. It applies the model on the data and returns the output. Parentid configuration is ignored in this stage.

```
{
  "type":"mlpredict",
  "parentIds":[],
  "taskIds":["61d69f9e-9284-4ef1-bbb0-7fc26e5e24cc"]
}
```
**Creating a pipeline**

Below steps are to be followed to create a pipeline.

1. Create load request to load customer data.
```
{
  "sourcetype":"csv",
  "name":"pipelinecustdata",
  "options": {
    "header" : "true",
    "path":"spark-tellius-project/spark_library/src/test/resources/testdata/fusionData/custData.csv"
  }
}
```
Response will be 
```
{
  "id": "customerdataloadid"
}
```

2. Create load request to load sales data.
```
{
  "sourcetype":"csv",
  "name":"pipelinesalesdata",
  "options": {
    "header" : "true",
    "path":"spark-tellius-project/spark_library/src/test/resources/testdata/fusionData/salesData.csv"
  }
}
```
Response will be 
```
{
  "id": "salesdataloadid"
}
```

3. Create transformation request on the customer data to project few columns.
```
{
  "sourceid" :"customerdataloadid",
  "transformationtype":"columnselect",
  "options":{"select":"true","columnnames":"customerid,name,age"}
}
```
Response will be
```
{
  "sourceid": "customerdataprojectid"
}
```

4. Create transformation request on the projected customer data to filter rows.
```
{
  "sourceid" :"customerdataloadid",
  "transformationtype":"filter",
  "options":{"condition":"name = 'peter'"}
}
```
Response will be
```
{
  "sourceid": "filtercustomerdata"
}
```


5. Create a Fusion request to perform inner join between customer data and sales data.
```
{
  "ids" :"filtercustomerdata,salesdataloadid",
  "datasetname" : "fusioned_dataset",
  "joinkeys1":"customerid",
  "joinkeys2":"customerid",
  "jointype":"inner",
  "columnnames":"name,transactionid,age"
}
```
Response will be
```
{
  "id": "salescustomerfusionid"
}
```


6. Create ML transform request to sample the joined output. The below request samples that data into 70% and 30%.
```
{ 
  "sourceid" :"salescustomerfusionid",
  "transformationtype":"sampleratio",
  "options":{"ratio":"0.7"}
}
```
Response will be
```
{
  "id": "sampleratio70percent",
  "subsetsourceId": ["sampleratio30percent"]
}
```
Where id field repesents data with the ratio 70%, and subsetsourceId field represents the data with the 30% ratio of data.

7. Creating a MLPipeline request for the 70% sample we created.
```
{
    "id" :"sampleratio70percent",
    "pipelineids":"$indexerId,$vectorAssemblerId,$trainId",
    "modelname":"pipelinemodel"
    "options":{
          "delay":"true"
    }
}
```
Response will be 
```
{
    "id": "mlpipelineregression"
}
```

8. Predicting the output for the 30% sample data.
```
{
      "id" :"sampleratio30percent",
      "modelname":"pipelinemodel"
}
```
Response will be 
```
{
      "id": "sample30predict"
}
```

9. Create save request to save the predicted 30% data.
```
{
          "sourceid":"sample30predict",
          "outputtype" :"txt",
          "options":{"header":"true",datasetname":"sampled30predicted","delay":"true", "path":"/home/ganesh/Desktop/text"}
}
```
Response will be
```
{
  "id": "save30percentdelayed",
}
```

10. Create a pipeline request using the above requests.
```
{
      "sourceid":"salesdataloadid",
      "name":"pipelinedocument"
      "stages":[{
            "type":"transform",
            "parentIds":[],
            "taskIds":["customerdataprojectid","filtercustomerdata"]
      },{
            "type":"fusion",
            "parentIds":[],
            "taskIds":["salescustomerfusionid"]
      },{
            "type":"splits",
            "parentIds":[]
            "taskIds":["sampleratio70percent","sampleratio30percent"]
      },{
            "type":"mltrain",
            "parentIds":[]
            "taskIds":["mlpipelineregression"]
      },{
            "type":"mlpredict",
            "parentIds":[]
            "taskIds":["sample30predict"]
      },{
            "type":"save",
            "parentIds":[]
            "taskIds":["save30percentdelayed"]
      }]
}
```
Response will be
```
{
      "id": "pipelinesalesdata",
      "name":"pipelinedocument"
}
```
We have customerdataloadid as the source for pipeline. This can be overridden in the execute api. Operations in the pipeline which were using customerdataloadid as the source will use the new id passed to the execute api as source. This enables the user to point the operations to a new source without updating the Pipeline request.

11. Parentids for each stage in the pipeline can also be specified. This configuration enables the requests inside that stage to use the parentids as their sourceid.
```
{
    "sourceid":"customerdataloadid"
    "name":"pipelinedocument"
    "stages":[{
          "type":"transform",
          "parentIds":[],
          "taskIds":["customerdataprojectid","filtercustomerdata"]
    },{
          "type":"fusion",
          "parentIds":["filtercustomerdata","newsalesdataloadid"],
          "taskIds":["salescustomerfusionid"]
    },{
          "type":"splits",
          "parentIds":[]
          "taskIds":["sampleratio70percent","sampleratio30percent"]
    },{
          "type":"mltrain",
          "parentIds":[]
          "taskIds":["mlpipelineregression"]
    },{
          "type":"mlpredict",
          "parentIds":[]
          "taskIds":["sample30predict"]
    },{
          "type":"save",
          "parentIds":[]
          "taskIds":["save30percentdelayed"]
    }]
}
```
Now that we have provided the parentids configuration to the fusion stage. it uses the new set of data for fusion. This enables the user to override the sourceid used while creating the request using Fusion API


Note:
  
1. Multiple requests in a transform stage will have each of its output piped to the next transform request.
2. Output of one stage will not be piped to another. This enables the user to reuse the output of a request in multiple stages.

**Pipeline execute**

This API is used to execute the pipeline which is already created on some input.

* Request URL : pipeline/execute
* Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | id of the pipeline which has to be executed| Yes |
| sourceid | source to the pipeline. Used to override the configuration provided while creating a pipeline| No |

Example:

```
{
  "id":"pipeLineRequestId",
  "sourceid":"newloadcustomerdataid"
}
```

Response will be,
```
{
  "id":"087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```
id represents the output data of the last task in last stage. It can be used to view the final output of pipeline.

**Pipeline View**

This API is used to view all the pipeline create requests.

* Request URL : pipeline/view
* Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| options | It will change based on what you are trying to list requests| No | 

Valid options are:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
|ids| It takes comma separated values of id's which you want to list, or "all" | No | |
|names| It takes comma separated values of names which you want to list, only load requests can be searched by name or "all" |No| |

Example : 

* search for all id's

```
{
  "options" :{ "ids" : "all"}
}
```

* search for all names

```
{
  "options" :{ "names" : "all"}
}
```

* search by a name

```
{
   "options" :{"names" : "nametest"}
}
```


* search by a id

```
{
   "options" : {"ids" : "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"}
}
```

Example Response:

```
{
  "results": [{
    "pipelineId": "b95d1c1e-a29d-4688-a8ed-71b78d24d66e",
    "parameters": {
      "sourceid":"somesourceid",
      "name":"test"
      "stages":[{
        "type":"load",
        "parentIds":[],
        "taskIds":["c6f60b41-2ccb-426a-8487-01e405f4fd87"]
        },{
        "type":"transform",
        "parentIds":[],
        "taskIds":["087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"]
        }]
      }
  },{
    "pipelineId": "b95d1c1e-a29d-4688-a8ed-71b78d24d66e",
    "parameters": {
      "sourceid":"somesourceid",
      "name":"test"
      "stages":[{
        "type":"load",
        "parentIds":[],
        "taskIds":["c6f60b41-2ccb-426a-8487-01e405f4fd87"]
        },{
        "type":"transform",
        "parentIds":[],
        "taskIds":["087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"]
        }]
      }
  }]
}
```

**Pipeline update

This API is used to view all the pipeline create requests.

* Request URL : pipeline/update
* Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | id for the pipeline request which has to be updated | Yes |
| sourceid | Optional parameter to specify a source to the pipeline. Can be overridden in Pipeline execute API | No |
| stages | Array of configuration for stages | Yes |

This API will assign the stages and name given in this request to the request associated with the value of id.

Example:

```
{
"sourceid":"somesourceid",
"id":"pipeLineRequestId",
"stages":[{
    "type":"transform",
    "parentIds":[],
    "taskIds":["c6f60b41-2ccb-426a-8487-01e405f4fd87","5c7cdf41-91e9-4f07-9075-b26ae0fd51d4"]
  },{
    "type":"fusion",
    "parentIds":[]
    "taskIds":["4b837d00-69b3-4e8f-9934-37a3219aa6cc"]
  },{
    "type":"save",
    "parentIds":[]
    "taskIds":["9d9e3c4d-da19-4424-90f6-fe0231550ca6"]
  }]
}
```

Response for this request will be,

```
{
"id":"087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```

**Pipeline Delete**

This API is used to delete pipeline request.

* Request URL : pipeline/delete
* Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | id of the pipeline create request which has to be deleted | Yes |

Example:

```
{
"id":"pipeLineRequestId"
}
```

Response

```
{
"id":"087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```

Schedule API
====================

This API is used to schedule pipeline request

* Request URL : api/schedule
* Parameters :

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | Pipeline id | Yes |
| name | Name assigned to the schedule flow | Yes |
| startdate |  Date on which task should be scheduled, format will be `MM/dd/yyyy`. | Yes |
| starttime | Time on which task should be scheduled. Format will be `mm,hh,pm/am,z` z represent timezone, (UDT/PDT etc..) (Timezone depends on the property default.timezone.id)| Yes |
| frequency | frequency in which it should be repeated. Takes a number which depends on frequencyunit | Yes|
| frequencyunit |Unit in which frequency is given | yes|
| noofretry | How many times the failed job needs to be retried, default value is 0 | No |
| timeout | Maximum time scheduled job can be running. Overrides the scheduledJobTimeOut configuration in application.conf | No |


* frequencyunit : Unit in which frequency is given, 

| Frequency Measure        | Value          |
| ------------- |:-------------:|
| Months | M |
| Weeks| w |
| Days | d |
| Hours | h |
| Minutes | m |
| Seconds | s |

* Scheduler related configuration in application.conf

| Configuratio name | Description |
| ------------- |:-------------:|
| numOfRetries | Number of times a failed scheduled job should be retried |
| retryBackoff| Time between each retry attempts (in milliseconds) |
| scheduledJobTimeOut | Timeout for a scheduled job to finish. This can be overridden for individual job in schedule request using timeout parameter (in milliseconds)|

Example:

For example in last example we got `6e80bbde-5ff8-4536-a6de-c611de69c512` as  response from transform has to be given as id for save request. But in save request `delay` should be set to true, so that execution doesnt happen at that time

Request URL : api/save
```
    {
              "sourceid":"6e80bbde-5ff8-4536-a6de-c611de69c512",
              "outputtype" :"json",
              "options":{"delay":"true", "path":"spark_library/src/test/resources/testdata/output", "datasetname":"saved_json"}
    }
```

Response will be like,
```
{
  "id": "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"
}
```
Response id from save has to be given to schedule request.
```
{
"id":"$saveRequestId",
"name":"flowname",
"startdate" :"04/25/2016",
"starttime":"04,30,pm,PDT",
"frequency":"2",
"frequencyunit":"M"
}
```


**Unschedule API**

This api is used to unschedule the scheduled task

* Request URL : /api/unschedule

This API takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | response id from the scheduled request | Yes |

Example : 

```
{
"id" :"$id"
}
```


**Schedule list**

This API is used to view all the schedule requests.

* Request URL : /api/schedule/list
* Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| options | It will change based on what you are trying to list requests| No | 

Valid options are:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
|ids| It takes comma separated values of id's which you want to list, or "all" | No | |
|names| It takes comma separated values of names which you want to list, only load requests can be searched by name or "all" |No| |

Example : 

* search for all id's

```
{
  "options" :{ "ids" : "all"}
}
```

* search for all names

```
{
  "options" :{ "names" : "all"}
}
```

* search by a name

```
{
   "options" :{"names" : "nametest"}
}
```


* search by a id

```
{
   "options" : {"ids" : "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"}
}
```


Response :

```
{
  "results": [
    "schedulerequestid": {
        "id":"087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b",
        "name":"flowname",
        "startdate" :"04/25/2016",
        "starttime":"04,30,pm,PDT",
        "frequency":"2",
        "frequencyunit":"M"
    }]
}
```

AGGREGATE API
====================

* Request URL : api/aggregate

Aggregate rest api takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id |The source id of the data.  | Yes |
| options     | These are options to be passed for aggregate |  Yes |

  * options :
    
 | Option Name        | Description          | Mandatory  | Default Value |
 | ------------- |:-------------:| -----:|---------:|
 | groupby |name of columns which should be grouped. Each column name should be separated by ",". In case of aggtype-timeseries only one date column has to be given. i.e Datecolumn on which timeseries function result needs to be founded. | Yes | Nothing |
 | aggregationcolumns |columns on which aggregation function has to be applied on, each columnname separated be ",". In case of aggtype-timeseries only one column can be given.|No | Nothing |
 | aaggtype-standard |Types of aggregation function. Each aggregation type is separated by `,` | Yes | Nothing |
 | aggtype-timeseries | timeseries function that has to be applied on the data | No | Nothing |
 | dateselector | Date on which timeseries function result needs to be founded (supports option today and yesterday) | Yes(In case of aggtype-timeseries) | Nothing |
 | filtercondition |Final condition on which columns needs to be selected.| No | Nothing |

* aggtype-standard : Valid values are,

        * count
        * sum
        * avg
        * median
        * max
        * min
        

* aggtype-timeseries : valid values are,
  - Peak-3hrs -> (PEAK_Hr1 , PEAK_Hr2, PEAK_Hr3)  
    + Peak_Hr1 is the  highest value in a hour for a certain KPI for the day
    + PEAK_Hr2 is the  2nd highest value in a hour for a certain KPI for the day
    + PEAK_Hr3 is the  3rd highest value in a hour for a certain KPI for the day
    + KPI is a function which will be used with a kPI  for example KPI1 = A+B/C.
  - Min_Hr -> Min_Hr(KPI1) for day, will find the lowest hourly aggregated value in that day for the KPI1
  - Avg-Peak3hrs -> average of Peak_Hr1(KPI1) , Peak_Hr2(KPI1) and Peak_Hr3(KPI1) for that date
  - Avg-Last7days -> average of daily KPI values for last 7 days excluding the date given
  - Avg-Last5Maxof7days -> average of daily KPI values for highest 5 out of last 7 days excluding date given
  - KPI1(today) -> KPI calculated with todays date
Example:

```
{
         "id" :"087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b",
         "options":{"groupby":"salary",
                    "aggregationtypes":"median,max",
                    "aggregationcolumns":"bonus",
                    "filtercondition":"median(bonus)=45 and max(bonus)=50"
                    }
}

Note: Name of the column will be <column_name>_<aggregation>. In this example, it will be bonus_max and bonus_median.
```


Example timeseries aggregation:

```javascript
{
"id" :"$id",
"options":{"groupby":"datevalue","dateselector":"2015-06-23","aggtype-timeseries":"Avg-Last7days","aggregationcolumns":"field1"}
}
```

INDICATOR API
====================

  Indicator API is used to create, update, delete, get indicator and apply an indicator on dataset.

### Create indicator:


  * Request URL : api/indicator
  * Request method : POST
  * Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| name | unique name to the indicator  | Yes |
| indicator | indicator AST which desrcibes the indicator functionality. |  Yes |

AST from Middleware is supported or AST can be created using the below documentation [Indicator AST format](#indicator-ast-format)

This API creates a new indicator and returns **indicatorId** which can be used to change the indicator or to apply it on a dataset.

##### Example : 

```javascript
{
    "name":"someindicatorname",
    "indicator" : {
        "type":"indicator",
        "value":{
            "type":"binary op",
            "op":"-",
            "value1":{
                "type":"column",
                "name":"amount",
                "datasetId":""
            },
            "value2":{
                "type":"column",
                "name":"discount",
                "datasetId":""
            }
        }
    }        
}
```

##### Response :

```javascript
{
    "name":"someindicatorname",
    "indicatorId":"someindicatorid"
}
```

### Update indicator:

  * Request URL : api/indicator/$indicatorId
  * Request method : PUT
  * Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| indicator | updated indicator AST |  Yes |

AST from Middleware is supported or AST can be created using the below documentation [Indicator AST format](#indicator-ast-format)

This API updates the new indicator AST to the indicator with id **$indicatorId**

##### Example : 

```javascript
{
    "indicator" : {
        "type":"indicator",
        "value":{
            "type":"binary op",
            "op":"-",
            "value1":{
                "type":"column",
                "name":"amount",
                "datasetId":""
            },
            "value2":{
                "type":"column",
                "name":"discount",
                "datasetId":""
            }
        }
    }        
}
```

##### Response :

```javascript
{
    "name":"someindicatorname",
    "indicatorId":"someindicatorid"
}
```


### List indicators:

  * Request URL : api/indicator
  * Request method : GET

This API lists the indicatorId and names of the indicators available in the system.

##### Response :

```javascript
{
    "indicators":[{
                    "indicatorId":"someindicatorid",
                    "name":"someindicator"
                  }, 
                  {
                    "indicatorId":"anotherindicatorid",
                    "name":"anotherindicator"
                  }]
}
```

### Get indicator:

  * Request URL : api/indicator/$indicatorId
  * Request method : GET

This API returns the AST of the indicator with id **$indicatorId**

##### Example:

```javascript
{
    "indicatorId":"someindicatorid",
    "name":"someindicatorname",
    "indicator" : {
        "type":"indicator",
        "value":{
            "type":"binary op",
            "op":"-",
            "value1":{
                "type":"column",
                "name":"amount",
                "datasetId":""
            },
            "value2":{
                "type":"column",
                "name":"discount",
                "datasetId":""
            }
        }
    }        
}
```  

### Delete indicator :

  * Request URL : api/indicator/$indicatorId
  * Request method : DELETE

This API deletes the indicator with id **$indicatorId** and returns true if the indicator was successfully deleted and false if failed to delete the indicator.

##### Response :

```javascript
{
    "deleted":"true"
}
```

### Apply indicator :

  * Request URL : api/indicator/apply
  * Request method : POST
  * Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | Source id of the data |  Yes |
| indicatorId | id of indicator which has to be applied on the data | Yes |
| columnname | name to the new column added by executing the indicator | Yes |


This API applies the indicator on the dataset and adds a new column with name **columnname** whose values would be the result of indicator execution.

It returns the id which 

##### Example :

```javascript
{
    "id":"sourcedataid",
    "indicatorId":"someindicatorid",
    "columnname":"discountedprice"
}
```

##### Response :

```javascript
{
    "id":"indicatorappliedid"
}
```

It returns the id of the data having indicator applied on to a new column with name **columnname**.


INDICATOR AST Format
====================

  Indicator AST is a JSON structure to specify the formula that needs to be applied on the dataframe.

  It can be one of the below types:

  * int
  * double
  * column
  * binary op

#### int

integer can be used to specify an integer in the formula or to set the column value as this value for all the rows.
  
```
{
  "value":{
      "value":100,
      "type":"int"
  },
  "type":"indicator"
}
```
  
#### double

double can be used to specify a double value in the formula or to set the column value as this value for all the rows.

```
  {
    "value":{
        "value":52.14,
        "type":"double"
    },
    "type":"indicator"
  }
```

#### column

column is used to specify a column value to be used in the formula or set the column value as the value for this new column.


```
  {
    "value":{
        "name":"salary",
        "type":"column"
    },
    "type":"indicator"
  }
```

#### binary op

binary operation is used to perform binary operations between any of the above indicators. 

```
{
  "value":{
      "type":"binary op",
      "op":"-",
      "value1":{
          "type":"column",
          "name":"price"
      },
      "value2":{
          "type":"column",
          "name":"discountedprice"
      }
  },
  "type":"indicator"
}
``` 
 Above AST adds a new column which would contain the value of price - discountedprice for that specific row.


  aggregation can also be done on a specific column, in which case the aggregated value would be added as the value for the new column.

```
  {
    "value":{
        "name":"salary",
        "aggregation":"max",
        "type":"column"
    },
    "type":"indicator"
  }
```

  aggregation field specifies the type of aggregation that has to be done on the column specified in **name** field.

  Supported aggregation types are:

| Aggregation       | Description          |
|:-----------------------:|:--------------------:|
| max | calculate the maximum value in column |
| min | calculate the minimum value in column |
| sum | calculate the sum of the values in column |
| count | count the number of values in the column |
| avg | calculate the average/mean of values in the column |
| stdev | calculate the standard deviation of the values in the column |

  aggregation can be **timeline restricted** to rows beloging to a specific timeline duration. 

```
{
  "value":{
      "name":"amount",
      "aggregation":"avg",
      "dateColumn":"transactiondate",
       "timeline":{
          "type":"today"
       }
      "type":"column"
  },
  "type":"indicator"
}
```
  The above AST would calculate the avg amount for today.
  Type specifies the type of timeline restriction that has to be done using the column specifed in **dateColumn** field.

  Supported timeline restriction types are :

| Timeline restriction type | Description | Parameters |
|:-------------------------:|:-----------:|:----------:|
| today | aggregate rows of todays date |  |
| yesterday | aggregate rows of yesterdays |  |
| last month | aggregate rows of last months |  |
| this month | aggregate rows of this months |  |
| last quarter | aggregate rows of last quarters |  |
| this quarter | aggregate rows of this quarters |  |
| last months | aggregate rows of last n months | value |
| last days | aggregate rows of last n days | value |

  parameters can be specified for timeline restriction as follows.

```
{
  "value":{
      "name":"amount",
      "aggregation":"avg",
       "dateColumn":"transactiondate",
       "timeline":{
          "type":"last months",
          "value":5
       }
      "type":"column"
  },
  "type":"indicator"
}
```
  Above AST would calculate average value of amount for last 5 months of data.


 All the above indicators can be nested or chained to calculate a complex equation

 Ex 1 : Equation to be calculated is ```amount - amount * ( discount / 100 )```

```
{
    "type":"indicator",
    "value":{
        "type":"binary op",
        "op":"-",
        "value1":{
            "type":"column",
            "name":"amount"
        },
        "value2":{
            "type":"binary op",
            "op":"*",
            "value1":{
                "type":"column",
                "name":"amount"
             },
             "value2":{
                "type":"binary op",
                "op":"/",
                "value1":{
                    "type":"column",
                    "name":"discount"
                 },
                 "value2":{
                    "type":"int",
                    "value":100
                 }
             }
        }
    }
}
```

Ex 2 : Equation to be calculated is ```stdev ( foo [ last 30 days ] ) + bar * ( -205.34 )```

```
{
    "type": "indicator",
    "value": {
        "type": "binary op",
        "op": "+",
        "value1": {
            "type": "column",
            "name": "foo"
            "dateColumn": "transactiondate",
            "timeline": {
                "type": "last days",
                "value": 30
            },
            "aggregation":"stdev"
        },
        "value2": {
            "type": "binary op",
            "op": "*",
            "value1": {
                "type": "column",
                "name": "bar"
            },
            "value2": {
                "type": "double",
                "value": -205.34
            }
        }
    }
}

```

SIGNATURE API
====================

  Signature API is used to create, update, delete, get signature and apply a signature on dataset.

### Create signature:


  * Request URL : api/signature
  * Request method : POST
  * Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| name | unique name to the signature  | Yes |
| signature | signature AST which describes the signature functionality. |  Yes |

AST from Middleware is supported or AST can be created using the below documentation [Signature AST format](#signature-ast-format)

This API creates a new signature and returns **signatureId** which can be used to change the signature or to apply it on a dataset.

##### Example : 

```javascript
 {
    "signature" : {
        "type":"signature",
        "value":{
            "if":{
                "op":">",
                "arg1":{
                    "value":{
                        "name":"salary",
                        "type":"column",
                        "datasetId":""
                    },
                    "type":"indicator"
                },
                "arg2":{
                    "value":5000,
                    "type":"int"
                },
                "type":"comparison"
            },
            "then":{
                "value":"Good salary",
                "type":"string"
            },
            "else":{
                "value":"Average salary",
                "type":"string"
            },
            "type":"if"
        }
    },
    "name":"$signatureName"
 }
```

##### Response :

```javascript
{
    "name":"somesignaturename",
    "signatureId":"somesignatureid"
}
```

### Update signature:

  * Request URL : api/signature/$signatureId
  * Request method : PUT
  * Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| signature | updated signature AST |  Yes |

AST from Middleware is supported or AST can be created using the below documentation [Signature AST format](#signature-ast-format)

This API updates the new signature AST to the signature with id **$signatureId**

##### Example : 

```javascript
 {
    "signature" : {
        "type":"signature",
        "value":{
            "if":{
                "op":">",
                "arg1":{
                    "value":{
                        "name":"salary",
                        "type":"column",
                        "datasetId":""
                    },
                    "type":"indicator"
                },
                "arg2":{
                    "value":5000,
                    "type":"int"
                },
                "type":"comparison"
            },
            "then":{
                "value":"Good salary",
                "type":"string"
            },
            "else":{
                "value":"Average salary",
                "type":"string"
            },
            "type":"if"
        }
    }
 }
```

##### Response :

```javascript
{
    "name":"somesignaturename",
    "signatureId":"somesignatureid"
}
```


### List signatures:

  * Request URL : api/signature
  * Request method : GET

This API lists the signatureId and names of the signatures available in the system.

##### Response :

```javascript
{
    "signatures":[{
                    "signatureId":"somesignatureid",
                    "name":"somesignature"
                  }, 
                  {
                    "signatureId":"anothersignatureid",
                    "name":"anothersignature"
                  }]
}
```

### Get signature:

  * Request URL : api/signature/$signatureId
  * Request method : GET

This API returns the signature AST of the signature with id **$signatureId**

##### Example:

```javascript
{
    "signatureId":"somesignatureid",
    "name":"somesignaturename",
    "signature" : {
        "type":"signature",
        "value":{
            "if":{
                "op":">",
                "arg1":{
                    "value":{
                        "name":"salary",
                        "type":"column",
                        "datasetId":""
                    },
                    "type":"indicator"
                },
                "arg2":{
                    "value":5000,
                    "type":"int"
                },
                "type":"comparison"
            },
            "then":{
                "value":"Good salary",
                "type":"string"
            },
            "else":{
                "value":"Average salary",
                "type":"string"
            },
            "type":"if"
        }
    }
 }
```  

### Delete signature :

  * Request URL : api/signature/$signatureId
  * Request method : DELETE

This API deletes the signature with id **$signatureId** and returns true if the signature was successfully deleted and false if failed to delete the signature.

##### Response :

```javascript
{
    "deleted":"true"
}
```

### Apply signature :

  * Request URL : api/signature/apply
  * Request method : POST
  * Parameters : 

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | Source id of the data |  Yes |
| signatureId | id of signature which has to be applied on the data | Yes |
| columnname | name to the new column added by executing the signature | Yes |


This API applies the signature on the dataset and adds a new column with name **columnname** whose values would be the result of signature execution.

It returns the id which 

##### Example :

```javascript
{
    "id":"sourcedataid",
    "signatureId":"somesignatureid",
    "columnname":"discountedprice"
}
```

##### Response :

```javascript
{
    "id":"signatureappliedid"
}
```

It returns the id of the data having signature applied on to a new column with name **columnname**.

Signature AST Format
====================
  
  Signature AST is a JSON structure to specify the if else condition that specifies the values for a new column to be added to the dataset.

  
  Signature can be used to specify a simple if else condition which would have only 2 unique outcomes.


```
{
  "value":{
      "type":"if",
      "if":{
          "type":"comparison",
          "op":">",
          "arg1":{
              "type":"int",
              "value":200
          },
          "arg2":{
              "type":"int",
              "value":300
          }
      },
      "then":{
          "type":"string",
          "value":"foo"
      },
      "else":{
          "type":"string",
          "value":"bar"
      }
  },
  "type":"signature"
}
```

  Simple signature involves 3 fields, if, then and else. if can be any expression returning a boolean.

  Above Signature AST is similar to ```if(200 > 300) 'foo' else 'bar'```

  **if** is a binaryoperation
  **then** and **else** is a value

  Multiple types of binaryoperation is supported.  It can be of following types:

  * comparison
  * equality
  * not
  * bool op

#### comparison

This is used to specify a comparison operator on 2 different values

```
{
    "type":"comparison",
    "op":">",
    "arg1":{
        "type":"int",
        "value":200
    },
    "arg2":{
        "type":"int",
        "value":300
    }

}
```
  Perform boolean operation specified by the field **op** on values **value1** and **value2**

  Above Signature AST is similar to ```if(200 > 300)```


#### equality

This is used to specify an equality condition on 2 different values. Similar to comparison, but the value for op is **=**


```
{
    "type":"equality",
    "arg1":{
        "type":"int",
        "value":200
    },
    "arg2":{
        "type":"int",
        "value":300
    }

}
```
  
  Above binary operation is similar to ```(200 = 300)```

#### not

This is used to specify the negation of other boolean operation. **boolexpression** field is used to specify any of the above booleanexpression.

```
{
    "type":"not",
    "value":{
        "type":"equality",
        "arg1":{
            "type":"int",
            "value":200
        },
        "arg2":{
            "type":"int",
            "value":300
        }
    }
}
```

  Above binary operation is similar to ```!(200 = 300)```

#### bool op

This is used to compare two boolean values. Valid values are AND, OR.


```
{
    "type":"bool op",
    "op":"AND",
    "value1":{
        "type":"equality",
        "value1":{
            "type":"int",
            "value":200
        },
        "value2":{
            "type":"int",
            "value":300
        },
    }
    "value2":{
        "type":"comparison",
        "op":">",
        "value1":{
            "type":"int",
            "value":500
        },
        "value2":{
            "type":"int",
            "value":300
        },
    }
}
```

Above binary operation is similar to ```(200 = 300) AND (500 > 300)```


  Multiple types of values are supported in the arg1, arg2, then, else etc

  * indicator
  * int 
  * double
  * string
  * boolean
  * bool

#### indicator

indicator can be specified as the value. It can be used to compare with other values and also specify an indicator for the thenvalue or elsevalue

```
{
    "type":"equality",
    "value1":{
        "name":"salary",
        "aggregation":"avg",
        "type":"column"
    },
    "value2":{
        "type":"int",
        "value":300
    }
}
```

  Above binary operation is similar to ```(avg(salary) = 300)```


#### int

int is used to specify a integer value for comparison or use it for thenvalue or elsevalue.

```
{
    "type":"int",
    "value":300
}
```

#### double

double is used to specify a double value for comparison or use it for thenvalue or elsevalue.

```
{
    "type":"double",
    "value":300.5
}
```

#### string 

string is used to specify a string for comparison or use it for thenvalue or elsevalue

```
{
    "type":"string",
    "value":"greaterthanaverage"
}
```

#### boolean 

boolean is used to specify a boolean for comparison or use it for thenvalue or elsevalue

```
{
    "type":"boolean",
    "value":false
}
```
  
#### bool 

booleanexpression can be specfied comparing two booleanexpression values or return a booleanexpression result as thenvalue or elsevalue


```
{
    "type":"bool",
    "value":{
        "type":"equality",
        "value1":{
            "name":"salary",
            "aggregation":"avg",
            "type":"column"
        },
        "value2":{
            "type":"int",
            "value":300
        }    
    }
}
```  

  Above value is similar to ```(avg(salary)=300)```

  Signture can also have else if cases which would have more than 2 unique outcomes.

  else if cases are specified as an array of conditions.

  Each elseif cases will have 2 fields **condition** and **then**

  Using elseIf we can chain multiple elseifconditions.

  **condition** is a binaryoperation
  **then** is a value


```
{
      "if":{
         "op":">",
         "arg1":{
            "value":{
               "name":"salary",
               "type":"column"
            },
            "type":"indicator"
         },
         "arg2":{
            "value":5000,
            "type":"int"
         },
         "type":"comparison"
      },
      "then":{
         "value":"Good salary",
         "type":"string"
      },
      "elseIf":[{
         "condition":{
            "op":"<",
            "arg1":{
               "value":{
                  "name":"salary",
                  "type":"column"
               },
               "type":"indicator"
            },
            "arg2":{
               "value":3000,
               "type":"int"
            },
            "type":"comparison"
         },
         "then":{
            "value":"Ok salary",
            "type":"string"
         }
      }],
      "else":{
          "value":"Average salary",
          "type":"string"
      }
      "type":"elseif"
   }
```

  Above Signature AST is similar to ```if salary > 5000 then 'Good salary' elseif salary < 3000 then 'Ok salary' else 'Average salary'```
    


FUSION API
====================

This API is used to join more than one datasource.

* Request URL : api/fusion

Fusion rest api takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| ids |The source id's of the data to be joined. These id's needs to be separated by ","  | Yes |
| datasetname |name to the fusioned dataset  | Yes |
| joinkeys1  | name of the key column(s) from first data source on which join should be performed. If join should happen on multiple key columns then separate them by ",". | Yes |
| joinkeys2  | name of the key column(s) from second data source corresponding to columns specified in order of joinkeys1 on which join should be performed. If join should happen on multiple key columns then separate them by ",".  | Yes |
| jointype     | Type of join. Valid values are `inner`,`outer`,`left_outer`,`right_outer` |  Yes |
| columnnames  | Name of the columns which has to be selected in the joined table or `all`  | Yes |

Since both the joining dataframes may have duplicate columns, which would results in error while selecting any column on the dataframe after join. The duplicate columns in the second dataframe will be renamed with a postfix **_2**.



Example1 : 

```
{
         "ids" :"$salesDataId,$custDataId",
         "datasetname" : "fusioned_dataset",
         "joinkeys1":"customerid",
         "joinkeys2":"customerid",
         "jointype":"inner",
         "columnnames":"name,transactionid,amount,age"
}
```

Example2 :
```
{
         "ids" :"$salesDataId,$custDataId",
         "datasetname" : "fusioned_dataset",
         "joinkeys1":"customerid",
         "joinkeys2":"customerid",
         "jointype":"right",
         "columnnames":"name,transactionid,amount,age"
}
```

Example3 : 
```
{
         "ids" :"$salesDataId,$custDataId",
         "datasetname" : "fusioned_dataset",
         "joinkeys1":"customerid,name",
         "joinkeys2":"customerid,cust_name",
         "jointype":"right",
         "columnnames":"transactionid,amount,age,cust_name"
}
```


List API
====================

List API is used to list all the load requests either by name, id or both.

* Request URL : api/list

List rest api takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| options | It will change based on what you are trying to list requests| No | 

Valid options are:

| Option Name        | Description          | Mandatory  | Default Value |
| ------------- |:-------------:| -----:|---------:|
|ids| It takes comma separated values of id's which you want to list, or "all" | No | |
|names| It takes comma separated values of names which you want to list, only load requests can be searched by name or "all" |No| |
| source | It tell what kind of source you wish to list. It takes values `dataset` , `datasource` , `all`| No | `all` |

Note:
Datasource indicates connection with external datastource for example Amazon S3, mongodB , HDFS source etc. Data is residing on an external source and is loaded when required
Dataset indicates anything which has been saved by the user within Tellius App - it can be any format - parquet, mongodB, memSQL , cassandra etc
Example:

To list request by id,

```
{
  "options": { "ids" : "087ce0a9-4d97-4e7b-aa69-581f8b0a2b3b"}
}
```

To list request by name,

```
{
  "options": { "names" : "request2"}
}
```

To list all the load request,

```
{
  "options": { "names" : "all"}
}
```

or

```
{
  "options": { "ids" : "all"}
}
```

Response: 

```
{
  "results": [{
    "name": "json data",
    "createdOn": "Wed Oct 07 18:09:02 IST 2015",
    "sourceType": "json",
    "size": 245163,
    "dataSourceName": "spark_library/src/test/resources/testdata/nestedJsonData/test.json",
    "id": "eff2bfba-72e6-4926-8e9a-90a9fdf7a86a",
    "schedule": ""
  }, {
    "name": "json data",
    "createdOn": "Wed Oct 07 18:09:02 IST 2015",
    "sourceType": "json",
    "size": 245163,
    "dataSourceName": "spark_library/src/test/resources/testdata/nestedJsonData/test.json",
    "id": "16cf8a56-a55a-43d1-999b-942e1dfe4986",
    "schedule": ""
  }]
}
```

----

**Note**

Size in the response is in KB.

----

Example for getting dataset:

Example:

```
{
  "options": { "ids" : "all",
                "source":"dataset"}
}
```

Response:

```
{
  "results": [{
    "name": "testsaveload",
    "createdOn": "Mon Feb 22 19:00:35 IST 2016",
    "sourceType": "csv",
    "size": 4096,
    "dataSourceName": "staging/output_Mon Feb 22 19:00:22 IST 2016/",
    "id": "aca0a582-94ba-4049-be98-d82aa1101b22",
    "schedule": ""
  }]
}
```

To get only datasource

```
{
  "options": { "ids" : "all",
                "source":"datasource"}
}
```


* Request URL : api/list/{id}

This API helps to get the details of the particular load requests by its id,

Request:

```
api/list/16cf8a56-a55a-43d1-999b-942e1dfe4986
```

Response:

```
{
    "sourcetype" :"csv",
    "name":"something",
    "options": {
      "header" : "true",
      "path":"spark_library/src/test/resources/testdata/airportdata/airports.csv"
    }
}
```


Delete API
====================

Delete API is used to delete the request based on its id.

* Request URL : api/delete

Delete rest api takes following parameters,

| Name        | Description          | Mandatory  |
| ------------- |:-------------:| -----:|
| id | Id of the load request (datasourceId or datasetId) which has to be deleted| No | 

Example:

```
{
  "id":"16cf8a56-a55a-43d1-999b-942e1dfe4986"
}
```

Response:

```
{
  "deleted": true
}
```

----

**Note**

When delete API is called source attached to the request is also deleted.

----

Config API
====================

Config API is used to get existing configuration or override the configuration provided in application.conf


* Request URL : api/config
* Request method : GET
    
This API returns all the configurations in Json format

```
{
    "tellius-project": {
        "azkaban-config":{ 
            "flowCreationDir":"/tmp/flows",
            "numOfRetries":5,
            "password":"azkaban",
            "project":"tellius",
            "retryBackoff":10000,
            "scheduledJobTimeOut":600000,
            "url":"http://localhost:8081",
            "username":"azkaban",
            "zipCreationDir":"/tmp"
        },
        "cache-config":{
            "cache-fusion":false,
            "cache-load":false,
            "cache-metadata":false,
            "cache-mltrain":false,
            "cache-predict":false,
            "storage-level":"MEMORY_ONLY"
        }
    }
```
    
    
* Request URL : api/config?key=tellius-project.cache-config
* Request method : GET
    
This API returns specific configuration for the key specified in query parameter

```
{
        "cache-fusion":false,
        "cache-load":false,
        "cache-metadata":false,
        "cache-mltrain":false,
        "cache-predict":false,
        "storage-level":"MEMORY_ONLY"
}
```

* Request URL : api/config
* Request method : POST
    
This API takes configuration in json format which needs to be updated and returns the updated configuration.
Json structure to update need to be provided from the root config 'tellius-project'

Example:

```
{
  "tellius-project" : {
    "cache-config" : {
        "cache-load" : true
    }
  }
}
```

Response:

```
{
    "tellius-project": {
        "azkaban-config":{ 
            "flowCreationDir":"/tmp/flows",
            "numOfRetries":5,
            "password":"azkaban",
            "project":"tellius",
            "retryBackoff":10000,
            "scheduledJobTimeOut":600000,
            "url":"http://localhost:8081",
            "username":"azkaban",
            "zipCreationDir":"/tmp"
        },
        "cache-config":{
            "cache-fusion":false,
            "cache-load":true,
            "cache-metadata":false,
            "cache-mltrain":false,
            "cache-predict":false,
            "storage-level":"MEMORY_ONLY"
        }
    }
```

Refresh API
====================

Refresh API is used to refresh the dataset. Subsequent operations after Refresh will use the latest available data.

* Request URL : api/load/refresh/$requestId
* Request method : POST

Response:

It returns whether refresh was successful or not

```
{
  "refreshed": true
}
```

**Specific Scenarios**

* Refreshing Load from httpurl
  
  Refreshing a dataset loaded from httpurl, redownloads the data from the httpurl when refresh its called. This can be used in cases where the data at the url is frequently updated. 


Application Configuration
=========================

**Spark configuration**

* spark-context
    * sparksql-env
        * **spark.sql.shuffle.partitions** : Number of partitions to use when shuffling data for joins or aggregations. Default value is 50
        * **spark.scheduler.mode** : Spark scheduler mode to schedule multiple jobs. Valid values are FIFO and FAIR.
        * **spark.scheduler.allocation.file** : FAIR scheduler configuration xml file path. File is present in conf/fairscheduler.xml
* scheduler-pool
    * **mlpool** : Poolname used to schedule Ml operations.
    * **savepool** : Poolname used to schedule Save operation.
    * **viewpool** : Poolname used to schedule View or Frontend based operation.


**Rest configuration**

* **server-hostname** = Hostname of the rest server. Default value is localhost
* **server-port** = Port of the rest server. Default value is 8080
* **save-pathprefix** = Location prefix where the data is saved from Save API. Default value is /tmp/saved_dataset/

**Cache configuration**

* **cache-metadata** : Enables metadata caching. This metadata includes key associated to some loaded dataframe, etc... Default value is false
* **cache-load** : Enables caching of loaded dataframe. Default value is false
* **cache-fusion** : Enables caching of fusioned dataframe. Default value is false
* **cache-mltrain** : Enables caching of input dataframe provided to ml train api. Default value is false.
* **cache-predict** : Enables caching of input dataframe provided to predict api. Default value is false.
* **storage-level** : This is a flag to tell where to persist data and you can refer [this](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence) for more details.
                                          



KAFKA INSTALLATION
====================

*Steps*

* Download the kafka from following link https://www.apache.org/dyn/closer.cgi?path=/kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz
* run command tar xzf kafka-<VERSION>.tgz
* cd kafka-<VERSION>
* ./sbt update
* ./sbt package
* ./sbt assembly-package-dependency

*Start the server*

* Open a console. Start the zookeeper from command `bin/zookeeper-server-start.sh config/zookeeper.properties`
* Open one more console. Start kafka server `bin/kafka-server-start.sh config/server.properties`

*Create the topic*

* Open one more console. Run command `bin/kafka-create-topic.sh --zookeeper localhost:2181 --replica 1 --partition 1 --topic <topicName>`

*Check what are the topics available*

* Run command `bin/kafka-list-topic.sh --zookeeper localhost:2181`

*Send some messages*

* Run command `bin/kafka-console-producer.sh --broker-list localhost:9092 --topic <topicName>`
* Once the above command is ran whatever the lines you write in that console will be the contents under that topic name.


**Test Load from kafkatopic name**

* Give load request with the following information.
    - sourcetype -> what will the type of messages in the topic
    - kafkatopicname -> topic name from which the data has to be saved
    - name -> Name given to the request
    - options -> which will have the path variable to which the data to be stored
* Schedule this request from the response id of the load request using schedule api

Example:

Load Request:

```
{
    "sourcetype":"csv",
    "kafkatopicname":"test"
    "options":{
      "path":"/home/ganesh/Projects/Analytics/spark-tellius-project/spark_library/src/test/resources/kafka/"
    }
}
```

Load Response:

```
{
  "id": "c84b9735-eb51-4efe-9acd-3275c67c3a28"
}
```

Schedule Request:
```
{
              "id":"c84b9735-eb51-4efe-9acd-3275c67c3a28",
              "startdate" :"29-10-2015",
              "starttime":"16:32",
              "frequency":"2",
              "frequencyunit":"MINUTE"
                
}
```

In the above example data in the topic will be saved into the folder given in path parameter in the interval scheduled in the scheduler.



AZKABAN INSTALLATION
====================

To install azkaban you need to get the code and build,

The following are the steps to get code and build.

* Clone code

`git clone https://github.com/azkaban/azkaban.git`

* Build

`./gradlew distZip`

* Copy from build

`cp build/distributions/azkaban-solo-server-3.0.0.zip ~`


***Installing solo server***

* Unzip 
`unzip ~/azkaban-solo-server-3.0.0.zip`

`cd ~/azkaban-solo-server-3.0.0`

* To change the timezone, change the property default.timezone.id in conf/azkaban.properties

`default.timezone.id=Asia/Kolkata`

* There is a issue in azkaban about the memory, to resolve it following steps needs to be done,
 
    - create a folder ~/azkaban-solo-server-3.0.0/plugins/jobtypes
    - create a file inside the folder jobtypes by name commonprivate.properties
    - copy memCheck.enabled=false inside the file.

* Starting solo server

`bin/azkaban-solo-start.sh`

* To check the logs

`tail -f logs/azkaban-execserver.log`

* To check the UI

`http://<hostname>:<port>`

By default the port is 8081.

* Create a project using UI

    - Login to azkaban by giving the username and password.
    
    Username : azkaban
    Password : azkaban
    
In the application.conf there is a property called `project` inside `azkaban-config`. Create a project by that name using the azkaban UI.


    * Click on button CreateProject
    * Give the name as tellius
    * Give some description
    * Click on CreateProject



STEPS TO ADD NEW FUNCTIONS TO OUR TOOL
====================

* To add a new function we have to start from adding a new model for corresponding API in rest module.
* Add model for the API in `com.tellius.rest.request.Models`.
    - For example, to add save functionality we need 2 models,
        + Save Request
        + Save Response
    - Example structure for save is 
        + case class SaveRequest(sourceid:String, outputtype:String, path:String, options: Option[Map[String,String]]) extends RestRequest 
        + case class SaveResponse(sourceId:String) extends RestResponse
    - Request api should extend RestRequest
    - Response api should extend RestResponse
* Match the rest request to the type of the request you sent in `com.tellius.rest.persistance.MongodbDataSourceTrackerUtils` class, in the method       `restRequestToMongoDBObject`.
* Above step needs to be done so that request can be converted to JSON format, and saved in mongodb.
    - For example - case SaveRequest(_,_,_,_) => restRequest.asInstanceOf[SaveRequest].toJson.toString()
* Match the mongodb object to RestRequest in class `com.tellius.rest.persistance.MongodbDataSourceTrackerUtils`, method `mongoDBObjectToRestRequest`
* Above step needs to be done so that Request object saved in mongo db as a JSON object has to be read and converted back to Request object.
    - For example - case "SaveRequest" => jsValue.convertTo[SaveRequest]
* Write logic for your API in request handler `com.tellius.rest.request`. In this implimentation logic of what has to be done based on the request can be written.
* These request handler implementations are the class which extends trait RequestHandler.
* Each implementation should override a method called `handleRequest(request: RestRequest): RestResponse`
* In these implementation will have validations and it will call the methods based on request parameters
* When we call resolveDf, this method will `resolveRequestedDF` method in `com.tellius.rest.persistance.DataSourceTrackerUtils`
* It will take the request and return the dataframe. 
* Methods which does the spark related operations should be written inside spark_library module. 
* Finally add referrence to your rest API in `com.tellius.rest.routing.RestService`
    - For example : 
        ```
        ~
          path("save") {
            crossDomain {
              options { ctx =>
                ctx.complete(StatusCodes.OK)
              } ~
                post {
                  entity(as[SaveRequest]) {
                    request => complete {
                      try {
                        logger.info("call save restservice api")
                        new SaveRequestHandler(restContext).handleRequest(request).asInstanceOf[SaveResponse]
                      } catch {
                        case inputException: java.lang.IllegalArgumentException => {
                          logger.error("error in save request")
                          logger.error(inputException.getMessage)
                          logger.error(inputException.getStackTraceString)
                          custom(400, inputException.getMessage)
                        }
                        case exception: Exception => {
                          logger.error("error in save request")
                          logger.error(exception.getMessage)
                          logger.error(exception.getStackTraceString)
                          custom(400, exception.getMessage)
                        }
                      }
                    }
                  }
                }
            }
          } 

        ```

* In this class request handler of the rest api has to be called based on path specified in the rest call.


**Datatype Mapping**

These are the some spark datatypes and its representation in the UI.

| Spark Datatype          | UI representation   |
|:-------------:|:-------------:|
| DoubleType | Numeric |
| FloatType | Numeric |
| IntType | Numeric |
| LongType | Numeric |
| ShortType | Numeric |
| String | Text |
| Date | Text |
